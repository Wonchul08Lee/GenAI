{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c0f3e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Etching 공정 전에는 반드시 세정 공정이 완료되어야 합니다.'),\n",
       " Document(metadata={}, page_content='PM 장비의 필터는 주 1회 정기적으로 교체해야 합니다.'),\n",
       " Document(metadata={}, page_content='웨이퍼 투입 전 챔버 내부의 온도 안정화가 필요합니다.'),\n",
       " Document(metadata={}, page_content='불량률이 2%를 초과할 경우 원인 분석 보고서를 제출해야 합니다.'),\n",
       " Document(metadata={}, page_content='클린룸 입장 전에는 반드시 정전기 방지복을 착용해야 합니다.'),\n",
       " Document(metadata={}, page_content='포토 공정 시 PR 코팅 두께는 1.5μm 이상 유지해야 합니다.'),\n",
       " Document(metadata={}, page_content='검사 장비는 매일 초기화 후 기능 점검을 실시합니다.'),\n",
       " Document(metadata={}, page_content='주간 생산 계획은 매주 월요일 오전 9시에 확정됩니다.'),\n",
       " Document(metadata={}, page_content='X선 검사 장비는 비정상 신호 발생 시 즉시 사용 중지합니다.'),\n",
       " Document(metadata={}, page_content='로더 설비의 진공 펌프는 월 1회 이상 윤활 상태를 점검합니다.'),\n",
       " Document(metadata={}, page_content='공정 이탈 발생 시 Shift 리더에게 즉시 보고합니다.'),\n",
       " Document(metadata={}, page_content='동일 Lot 내에서 불량이 5개 이상 발생하면 전수 검사 실시합니다,'),\n",
       " Document(metadata={}, page_content='물류 이송 로봇은 경로 장애 감지 시 자동 우회합니다.'),\n",
       " Document(metadata={}, page_content='이온 주입 공정 후 열처리 시간은 최소 30분 이상 확보합니다.'),\n",
       " Document(metadata={}, page_content='클린룸 청소는 일 2회, 장비 청소는 주 1회 이상 실시합니다.'),\n",
       " Document(metadata={}, page_content='신입 엔지니어는 공정별 교육을 모두 이수한 후 장비 조작이 가능합니다.'),\n",
       " Document(metadata={}, page_content='공정 레시피 변경 시 Change History 문서 작성을 필수로 합니다.'),\n",
       " Document(metadata={}, page_content='수율 개선안은 월간 품질 회의에서 발표되어야 합니다.'),\n",
       " Document(metadata={}, page_content='Mask alignment 오류가 0.2μm를 초과하면 장비 점검을 실시합니다.'),\n",
       " Document(metadata={}, page_content='MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.'),\n",
       " Document(metadata={}, page_content='웨이퍼 저장 캐리어는 주기적으로 오염도를 측정해야 합니다.'),\n",
       " Document(metadata={}, page_content='장비 재가동 시 Warm-up 절차를 반드시 이행합니다.'),\n",
       " Document(metadata={}, page_content='Test wafer는 생산 wafer 투입 전에 반드시 시뮬레이션합니다.'),\n",
       " Document(metadata={}, page_content='제조 Lot ID는 공정마다 바코드로 자동 추적됩니다.'),\n",
       " Document(metadata={}, page_content='소자 특성 분석 결과는 전자문서 시스템에 등록합니다.'),\n",
       " Document(metadata={}, page_content='야간조 작업자는 작업 시작 전 점검 체크리스트를 반드시 확인합니다.'),\n",
       " Document(metadata={}, page_content='공정 조건이 기준에서 벗어난 경우 자동 알람이 발생합니다.'),\n",
       " Document(metadata={}, page_content='생산 중단 요청은 생산 관리팀 승인 후 진행할 수 있습니다.'),\n",
       " Document(metadata={}, page_content='사내 기술 포럼 자료는 R&D 인트라넷에 주기적으로 업데이트됩니다.'),\n",
       " Document(metadata={}, page_content='정전 발생 시 UPS 시스템으로 30분간 운영이 가능합니다.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# langchain + RAG\n",
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "\n",
    "emdeding_id = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "qa_model_id = \"monologg/koelectra-base-v3-finetuned-korquad\"\n",
    "tokenizer_opt = {\n",
    "    \"max_length\": 512, \n",
    "    \"truncation\": True,\n",
    "}\n",
    "\n",
    "df = pd.read_csv(\"C://Users//Leo//code//ch09_랭체인_기초//test02//data/data.csv\", encoding=\"utf-8\")\n",
    "texts = df[\"text\"].tolist()\n",
    "docs = [Document(page_content=text) for text in texts]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b90cbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "References : 공정 이탈 발생 시 Shift 리더에게 즉시 보고합니다.\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name = emdeding_id)\n",
    "save_vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chromaDB1\"\n",
    ")\n",
    "load_vectordb = Chroma(\n",
    "    persist_directory=\"./chromaDB1\",\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "question = \"클린룸 입장시 주의 사항 알려줄래?\"\n",
    "retriever = load_vectordb.as_retriever(search_kwargs={\"k\":3})\n",
    "relevant_docs = retriever.invoke(question)\n",
    "best_context = relevant_docs[0].page_content\n",
    "print(\"References :\", best_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31707a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='f6f7f7d6-28c1-4b64-a09d-0dd5925f4f68', metadata={}, page_content='공정 이탈 발생 시 Shift 리더에게 즉시 보고합니다.'),\n",
       " Document(id='e8d38da8-a23d-4ee5-afa4-0f4c4aa5a40e', metadata={}, page_content='공정 이탈 발생 시 Shift 리더에게 즉시 보고합니다.'),\n",
       " Document(id='b1184416-11d9-4925-ab2d-9eb2c6553b6e', metadata={}, page_content='공정 이탈 발생 시 Shift 리더에게 즉시 보고합니다.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc564c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer : 공정 이탈 발생 시 Shift 리더에게 즉시 보고합니다\n"
     ]
    }
   ],
   "source": [
    "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_id)\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_id)\n",
    "text_gen = pipeline(\n",
    "    task=\"question-answering\",\n",
    "    model=qa_model,\n",
    "    tokenizer=qa_tokenizer,\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    do_sample=True,\n",
    "    device = -1\n",
    ")\n",
    "\n",
    "result = text_gen(question=question, context=best_context)\n",
    "print(\"Answer :\", result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c93cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: 공정 이탈 발생 시 Shift 리더에게 즉시 보고합니다.\n",
      "Answer: 공정 이탈 발생 시 Shift 리더에게 즉시 보고합니다\n"
     ]
    }
   ],
   "source": [
    "# 앞에것과 동일하지만, transformer 미사용\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "emdeding_id = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "qa_model_id = \"monologg/koelectra-base-v3-finetuned-korquad\"\n",
    "tokenizer_opt = {\n",
    "    \"max_length\": 512, \n",
    "    \"truncation\": True,\n",
    "}\n",
    "\n",
    "df = pd.read_csv(\"C://Users//Leo//code//ch09_랭체인_기초//test02//data/data.csv\", encoding=\"utf-8\")\n",
    "texts = df[\"text\"].tolist()\n",
    "docs = [Document(page_content=text) for text in texts]\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=emdeding_id)\n",
    "save_vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chromaDB2\"\n",
    ")\n",
    "load_vectordb = Chroma(\n",
    "    persist_directory=\"./chromaDB2\",\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    task=\"question-answering\",\n",
    "    model=qa_model_id,\n",
    "    tokenizer_kwargs=tokenizer_opt,\n",
    "    device = -1\n",
    ")\n",
    "\n",
    "question = \"클린룸 입장 시 주의사항 알려줄래?\"\n",
    "retriever = load_vectordb.as_retriever(search_kwargs={\"k\":1})\n",
    "relevant_docs = retriever.invoke(question)\n",
    "best_context = relevant_docs[0].page_content\n",
    "print(\"참고 문서:\", best_context)\n",
    "\n",
    "result = qa_pipeline(\n",
    "    question=question,\n",
    "    context=best_context\n",
    ")\n",
    "print(\"Answer:\", result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d307da49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "References : 공정 이탈 발생 시 Shift 리더에게 즉시 보고합니다.\n"
     ]
    }
   ],
   "source": [
    "# GEN 추가\n",
    "\n",
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AutoModelForCausalLM, pipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "embedding_id = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "qa_model_id = \"monologg/koelectra-base-v3-finetuned-korquad\"\n",
    "qa_tokenizer_opt = {\n",
    "    \"max_length\": 512,  \n",
    "    \"truncation\": True,\n",
    "}\n",
    "gen_model_id = \"EleutherAI/polyglot-ko-1.3b\"\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=embedding_id)\n",
    "load_vectordb = Chroma(\n",
    "    persist_directory=\"./chromaDB2\",\n",
    "    embedding_function=embedding_model\n",
    ")   \n",
    "\n",
    "question = \"클린룸 입장 시 주의사항 알려줄래?\"\n",
    "retriever = load_vectordb.as_retriever(search_kwargs={\"k\":3})\n",
    "relevant_docs = retriever.invoke(question)\n",
    "best_context = relevant_docs[0].page_content\n",
    "print(\"References :\", best_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e0bd6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer : 공정 이탈 발생 시 Shift 리더에게 즉시 보고합니다\n"
     ]
    }
   ],
   "source": [
    "# KoElectra QA 모델 로드\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_id)\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_id)\n",
    "\n",
    "text_gen = pipeline(\n",
    "    task=\"question-answering\",\n",
    "    model=qa_model,\n",
    "    tokenizer=qa_tokenizer,\n",
    "    tokenizer_kwargs=qa_tokenizer_opt,\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    do_sample=False,\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "result = text_gen(question=question, context=best_context)\n",
    "print(\"Answer :\", result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "003d74e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ed24c73c314192af413cd57f914fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자연어 응답:\n",
      "\n",
      "너는 똑똑한 제조 전문가야. 아래의 질문과 정답을 바탕으로 자연스럽고 친절하게 응답해줘.\n",
      "    Question: 클린룸 입장 시 주의사항 알려줄래?\n",
      "    Answer: 공정 이탈 발생 시 Shift 리더에게 즉시 보고합니다\n",
      "    자연어 응답:\n",
      "    Question: 클린룸에서 공정 이탈 시 어떻게 해야 하는지 알려줄래?    Answer: 공정 이탈 발생 시 Shift 리더에게 즉시 보고합니다.    Question: 클린룸에서 공정 이탈 시 어떻게 해야 하는지 알려줄래?    Answer: 공정 이탈 발생 시 Shift 리더에게 즉시 보고합니다.    Question: 클린룸에서 공정 이탈 시 어떻게 해야 하는지 알려줄래?    Answer: 공정 이탈 발생 시\n"
     ]
    }
   ],
   "source": [
    "gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_id)\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(gen_model_id)\n",
    "gen_pipeline = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=gen_model,\n",
    "    tokenizer=gen_tokenizer,\n",
    "    max_length=200,\n",
    "    truncation=True,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    device=-1\n",
    ")\n",
    "gen_llm = HuggingFacePipeline(pipeline=gen_pipeline)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "너는 똑똑한 제조 전문가야. 아래의 질문과 정답을 바탕으로 자연스럽고 친절하게 응답해줘.\n",
    "    Question: {question}\n",
    "    Answer: {answer}\n",
    "    자연어 응답:\n",
    "\"\"\")\n",
    "chain = prompt|gen_llm\n",
    "\n",
    "final_response = chain.invoke({\n",
    "    \"question\": question,\n",
    "    \"answer\": result['answer']\n",
    "})\n",
    "print(\"자연어 응답:\")\n",
    "print(final_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f9f4e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# RAG + GEN + History\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AutoModelForCausalLM, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from uuid import uuid4\n",
    "\n",
    "model_id = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "text_gen = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    truncation=False,\n",
    "    do_sample=True,\n",
    "    return_full_text=False,\n",
    "    temperature=0.7,\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=text_gen)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"지금까지의 이야기:\\n{history}\\n\\n이제 '{animal}'에 대한 이야기를 써 주세요:\\n\"\n",
    ")\n",
    "chat_histories = {}\n",
    "def get_history(session_id):\n",
    "    if session_id not in chat_histories:\n",
    "        chat_histories[session_id] = ChatMessageHistory()\n",
    "    return chat_histories[session_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e6c6a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    sesseion_id = str(uuid4())\n",
    "    print(\"동물 이름을 입력해 이야기를 시작하세요.\")\n",
    "\n",
    "    while True:\n",
    "        animal = input(\"\\n 동물 이름 또는 질문 입력(종료하려면 '종료'): \").strip()\n",
    "        if animal in [\"종료\", \"exit\", \"quit\"]:\n",
    "            print(\"이야기를 종료합니다.\")\n",
    "            break\n",
    "\n",
    "        history = get_history(sesseion_id)\n",
    "\n",
    "        if history.messages:\n",
    "            history_text = \"\\n\".join(\n",
    "                f\"{msg.type.upper()}: {msg.content}\" for msg in history.messages)\n",
    "            prompt_input = prompt.format(history=history_text, animal=animal)\n",
    "        else:\n",
    "            prompt_input = f\"'아주 먼 옛날 {animal} 한 마리가 살고 있었습니다.그러던 어느날'\"\n",
    "\n",
    "        final_response = llm.invoke(prompt_input)\n",
    "        print(f\"\\n생성된 이야기:\\n{final_response}\")\n",
    "\n",
    "        history.add_user_message(animal)\n",
    "        history.add_ai_message(final_response)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03851c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동물 이름을 입력해 이야기를 시작하세요.\n",
      "\n",
      "생성된 이야기:\n",
      " 그 토끼를 본 토끼의 어머니는 깜짝 놀라 그 토끼의 머리를 물어뜯어 물었습니다.\" 토끼는 그 토끼의 목을 물어뜯어 죽인 후에 그 토끼의 머리에다 물을 뿌렸습니다.\n",
      "이때 그 토끼는 그 토끼의 머리 위로 올라가서 그 토끼의 머리 위에 올라가서 그 토끼의 머리를 물어뜯었습니다.\n",
      "이처럼 토끼의 머리 위에 올라서 물리는 물고기의 습성은, 그 물고기의 습성이 물릴 때 그 물고기의 습성을 씻어 주는 습성을 담고\n",
      "\n",
      "생성된 이야기:\n",
      "\"우리는 '사자'에 대한 이야기를 잘 쓰기 위해서는, 그 물고기의 습성을 풀어 내야 합니다.\"\n",
      "1. '사자'에 대하여 이야기하기 위해서는, 그 물고기의 습성을 풀어 주어야 합니다.\n",
      "그 물고기의 습성은, 그 물고기의 습성이 어떤 것인지 잘 알기 위해서는, 그 물고기의 습성이 어떤 것인지 잘 알고 있어야 합니다.\n",
      "2. 물고기에게 말을 걸어 볼 때에는, 그 물고기의 습성이 어떤 것인지 잘 설명하고 있어야 합니다.\n",
      "3. 물고기의 습성이 어떻게 되는지를 알아내려면, 그 물고기의\n",
      "\n",
      "생성된 이야기:\n",
      "\"고양이에 대해서 이야기해 주세요.\"\n",
      "\"고양이에게 말을 걸어 볼 때에는, 그 물고기의 습성이 어떤 것인지 잘 이해해야 합니다.\"\n",
      "\"고양이에게 말을 걸어 볼 때에는, 그 물고기의 습성이 어떤 것인지 잘 알고 있어야 합니다.\"\n",
      "3. 물고기에게 말을 걸어 볼 때에는, 그 물고기의\n",
      "이 물음에 대답해 주어야 합니다.\n",
      "\"고양이에 대해서 이야기해 주세요.\"\n",
      "4. 물고기의\n",
      "이 물음에 대답해 주십시오.\n",
      "\"고양이에게 말을 걸어 볼\n",
      "이야기를 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI2503",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
