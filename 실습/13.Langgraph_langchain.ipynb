{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879122e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아주 먼 옛날 토끼 이 살고 있었습니다. 그러던 어느날 토끼를 한 마리 잡아서 우리 집으로 데려다가 줬는데 그 토끼가 토끼가 아니라 토끼라고 해서 토끼가 아니었다는 사실 기억도 못합니다. 아~ 토끼가 아니라 토끼로 생각하시면 안 됩니다. 그~ 토끼가 토끼였죠. 아니 토끼가 아니니까 토끼라고 생각하시면 안 됩니다. 토끼도 아니니까 토끼입니다 토끼입니다 토끼입니다 토끼입니다 토끼입니다 토끼입니다 토끼입니다 토끼입니\n",
      "아주 먼 옛날 용 이 살고 있었습니다. 그러던 어느날 님이 우리 할아버지와 그 집 아저씨한테서 와 와 와 와 와 와 와 와 와 와 와 와 와 를 데리고 나가셨습니다. 그때 님이 그 집 아주머니께 와 와 와 와 와 와 와 와 와 와 와 \n",
      "응답: 장편소설\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_id = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "text_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    truncation=False,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    return_full_text= True,\n",
    "    temperature=0.7)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_gen)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"아주 먼 옛날 {user_input} 이 살고 있었습니다. 그러던 어느날\")\n",
    "chain = prompt| llm\n",
    "result1 = chain.invoke({\"user_input\": \"토끼\"})\n",
    "print(result1)\n",
    "len(result1)\n",
    "result2 = chain.invoke({\"user_input\": \"용\"})\n",
    "print(result2)\n",
    "len(result2)\n",
    "\n",
    "\n",
    "#1. 분류 노드\n",
    "def classification(state):\n",
    "    text = state.get(\"user_input\", \"\")\n",
    "    result = chain.invoke({\"user_input\": text}).strip()\n",
    "\n",
    "    if (len(result) > 250):\n",
    "        label = \"long_answer\"\n",
    "    else: \n",
    "        label = \"short_answer\"\n",
    "    return {**state, \"label\": label}\n",
    "\n",
    "#2. 응답 노드\n",
    "def long_answer(state):\n",
    "    return {**state, \"response\": \"장편소설\"}\n",
    "def short_answer(state):\n",
    "    return {**state, \"response\": \"단편 소설\"}\n",
    "\n",
    "def get_label(state):\n",
    "    return state.get(\"label\", \"\")\n",
    "\n",
    "graph = StateGraph(dict)\n",
    "graph.add_node(\"classification\", classification)\n",
    "graph.add_node(\"long_answer\", long_answer)\n",
    "graph.add_node(\"short_answer\", short_answer)\n",
    "graph.set_entry_point(\"classification\")\n",
    "\n",
    "graph.add_conditional_edges(\"classification\", get_label, {\n",
    "    \"long_answer\": \"long_answer\",\n",
    "    \"short_answer\": \"short_answer\"\n",
    "})\n",
    "\n",
    "graph.add_edge(\"long_answer\", END)\n",
    "graph.add_edge(\"short_answer\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "user_input = input(\"동물 입력: \")\n",
    "final_state = app.invoke({\"user_input\": user_input})\n",
    "print(\"응답:\", final_state.get(\"response\", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57537827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "장편소설\n",
      "\n",
      "이전에 생성된 이야기 보여줘\n",
      " 아주 먼 옛날 토끼 이 살고 있었습니다. 그러던 어느날 토끼가 찾아왔습니다. 그때 토끼는 곧 그 친구에게 편지를 썼습니다. 그러자 친구가 토끼가 무슨 일을 하는지 물어보면서 토끼는 그 친구가 바로 그 친구를 위해 일한다는 것이 무슨 뜻인지 물어보았습니다. 그러자 토끼는 그 친구를 위해 일한다는 것이 무슨 뜻인지 물었습니다. 그러자 그 친구가 그 친구에게 편지를 써서 보냈습니다. 그 친구가 그 편지를 그 친구에게 보낸 바로 다음날이었습니다. 그 날 토끼는 그 친구에게 편지를 전해주었는데 그 친구는 그 친구를 위해 일을 한다는\n",
      "\n",
      "대화 종료\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_id = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "text_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    truncation=False,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    return_full_text= True,\n",
    "    temperature=0.7)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_gen)\n",
    "prompt = PromptTemplate.from_template(\"아주 먼 옛날 {user_input} 이 살고 있었습니다. 그러던 어느날\")\n",
    "chain = prompt| llm\n",
    "\n",
    "\n",
    "#1. 분류 노드\n",
    "def classification(state):\n",
    "    text = state.get(\"user_infput\", \"\")\n",
    "    result = chain.invoke({\"user_input\": user_input}).strip()\n",
    "\n",
    "    if (len(result) > 250):\n",
    "        label = \"long_answer\"\n",
    "    else: \n",
    "        label = \"short_answer\"\n",
    "    return {**state, \"label\": label, \"story\": result}\n",
    "\n",
    "#2. 응답 노드\n",
    "def long_answer(state):\n",
    "    res = \"장편소설\"\n",
    "    print(res)\n",
    "    return {**state, \"response\": res}\n",
    "def short_answer(state):\n",
    "    res = \"단편 소설\"\n",
    "    print(res)\n",
    "    return {**state, \"response\": res}\n",
    "\n",
    "def ask_continue(state):\n",
    "    reply = input(\"\\n추가 질문하시겠습니까? (y/n): \").strip()\n",
    "    if reply in [\"예\", \"네\", \"y\"]:\n",
    "        state[\"continue\"] = True\n",
    "        qestion = input(\"추가 질문을 입력하세요: \")\n",
    "        state[\"user_input\"] = qestion\n",
    "    else:\n",
    "        state[\"continue\"] = False\n",
    "    return state\n",
    "\n",
    "def shuld_continue(state):\n",
    "    if state.get(\"continue\"):\n",
    "       answer = \"replay\"\n",
    "    else:\n",
    "       answer= \"__end__\"\n",
    "    return answer\n",
    "\n",
    "def replay_story(state):\n",
    "    question= state.get(\"user_input\", \"\")\n",
    "    if \"보여줘\" in question :\n",
    "        print(\"\\n이전에 생성된 이야기 보여줘\\n\", state.get(\"story\", \"이야기가 없습니다.\"))\n",
    "        return {**state, \"response\": \"이야기 재생성 완료\"}\n",
    "    else:\n",
    "        return classification(state)    \n",
    "\n",
    "\n",
    "def get_label(state):\n",
    "    return state.get(\"label\", \"\")\n",
    "\n",
    "graph = StateGraph(dict)\n",
    "graph.add_node(\"classification\", classification)\n",
    "graph.add_node(\"long_answer\", long_answer)\n",
    "graph.add_node(\"short_answer\", short_answer)\n",
    "graph.add_node(\"ask_continue\", ask_continue)\n",
    "graph.add_node(\"replay_story\", replay_story)\n",
    "\n",
    "graph.set_entry_point(\"classification\")\n",
    "\n",
    "graph.add_conditional_edges(\"classification\",get_label, {\n",
    "                            \"long_answer\":\"long_answer\",\n",
    "                            \"short_answer\":\"short_answer\"\n",
    "})\n",
    "graph.add_edge(\"long_answer\", \"ask_continue\")\n",
    "graph.add_edge(\"short_answer\", \"ask_continue\")\n",
    "\n",
    "graph.add_conditional_edges(\"ask_continue\", shuld_continue, {\n",
    "    \"replay\": \"replay_story\",\n",
    "    \"__end__\": END\n",
    "})\n",
    "\n",
    "graph.add_edge(\"replay_story\", \"ask_continue\")    \n",
    "\n",
    "app = graph.compile()\n",
    "user_input = input(\"동물 입력: \")\n",
    "finanl_state = app.invoke({\"user_input\": user_input})\n",
    "print(\"\\n대화 종료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbd3c981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "답변: Human:  다음 문맥을 사용하여 질문에 답변하세요.\n",
      "답을 모르면 모른다고 말하세요.\n",
      "간결하게 답변하세요\n",
      "\n",
      "문맥: MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.\n",
      "\n",
      "MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.\n",
      "\n",
      "MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.\n",
      "\n",
      "MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.\n",
      "질문: MES 공정 로그 보관 기간은?\n",
      "답변: MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.\n",
      "답변: MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.\n",
      "질문: MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.\n",
      "질문: MES시스템에 기록된 공정 로그는 6개월간 보관됩니다.\n",
      "질문: MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.\n",
      "질문: MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.\n",
      "질문: MES 시스템\n",
      "\n",
      "[참고 문서]:\n",
      "1. MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다....\n",
      "2. MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다....\n",
      "3. MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다....\n",
      "4. MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'MES 공정 로그 보관 기간은?',\n",
       " 'answer': 'Human:  다음 문맥을 사용하여 질문에 답변하세요.\\n답을 모르면 모른다고 말하세요.\\n간결하게 답변하세요\\n\\n문맥: MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.\\n\\nMES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.\\n\\nMES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.\\n\\nMES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.\\n질문: MES 공정 로그 보관 기간은?\\n답변: MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.\\n답변: MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.\\n질문: MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.\\n질문: MES시스템에 기록된 공정 로그는 6개월간 보관됩니다.\\n질문: MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.\\n질문: MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.\\n질문: MES 시스템',\n",
       " 'source_docs': [Document(id='95b91eb1-6210-4bae-b3f4-d5be1883c29c', metadata={}, page_content='MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.'),\n",
       "  Document(id='c607a805-3af3-4491-9915-e6e338c003e2', metadata={}, page_content='MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.'),\n",
       "  Document(id='e3268b9a-f08a-492b-9326-6dbaf4855fed', metadata={}, page_content='MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.'),\n",
       "  Document(id='a267291c-1e50-4ac4-a92e-3f7967825f6a', metadata={}, page_content='MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.')],\n",
       " 'see_reference': 'y',\n",
       " 'continue': 'n'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#langgraph + lanchain + RAG\n",
    "\n",
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "df = pd.read_csv('C:\\\\Users\\\\Leo\\\\code\\\\ch07_트랜스포머_RAG\\\\test03\\\\data\\\\data.csv', encoding='utf8')\n",
    "df.head(8)\n",
    "\n",
    "texts = df['text'].tolist()\n",
    "docs = [Document(page_content=text) for text in texts]\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chromaDB\"\n",
    ")\n",
    "\n",
    "# LLM\n",
    "model_id = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "text_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    truncation=False,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    return_full_text= True,\n",
    "    temperature=0.7)\n",
    "\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_gen)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "template = \"\"\" 다음 문맥을 사용하여 질문에 답변하세요.\n",
    "답을 모르면 모른다고 말하세요.\n",
    "간결하게 답변하세요\n",
    "\n",
    "문맥: {context}\n",
    "질문: {question}\n",
    "답변:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever| format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def ask_question(state):\n",
    "    question = input(\"질문을 입력해주세요:\")\n",
    "    retieved_docs = retriever.invoke(question)\n",
    "    answer = rag_chain.invoke(question)\n",
    "\n",
    "    res = {\n",
    "        \"question\": question,        \n",
    "        \"answer\": answer,\n",
    "        \"source_docs\": retieved_docs\n",
    "    }\n",
    "    return res\n",
    "\n",
    "def get_answer(state):\n",
    "    print(\"\\n답변:\", state[\"answer\"])\n",
    "    return state\n",
    "\n",
    "def ask_reference(state):\n",
    "    reply = input(\"\\n참고 문서를 보겠습니까? (y/n): \").strip()\n",
    "    return {**state, \"see_reference\":reply}\n",
    "\n",
    "def get_reference(state):\n",
    "    print(\"\\n[참고 문서]:\")\n",
    "    for i, doc in enumerate(state[\"source_docs\"], 1):\n",
    "        print(f\"{i}. {doc.page_content[:200]}...\")\n",
    "    return state\n",
    "\n",
    "def ask_continue(state):\n",
    "    reply = input(\"\\n추가 질문하시겠습니까? (y/n): \").strip()\n",
    "    return {**state, \"continue\": reply}\n",
    "  \n",
    "def reference_or_not(state):\n",
    "    return \"get_reference\" if state.get(\"see_reference\",\"\") in [\"예\",\"네\",\"y\"] else \"ask_continue\"\n",
    "\n",
    "def continue_or_not(state):\n",
    "    return \"ask_question\" if state.get(\"continue\",\"\") in [\"예\",\"네\",\"y\"] else END\n",
    "\n",
    "graph = StateGraph(dict)\n",
    "graph.add_node(\"ask_question\", ask_question)\n",
    "graph.add_node(\"get_answer\", get_answer)\n",
    "graph.add_node(\"ask_reference\", ask_reference)\n",
    "graph.add_node(\"get_reference\", get_reference)\n",
    "graph.add_node(\"ask_continue\", ask_continue)\n",
    "\n",
    "graph.set_entry_point(\"ask_question\")\n",
    "\n",
    "graph.add_edge(\"ask_question\", \"get_answer\")\n",
    "graph.add_edge(\"get_answer\", \"ask_reference\")\n",
    "graph.add_conditional_edges(\"ask_reference\", reference_or_not, {\n",
    "    \"get_reference\": \"get_reference\",\n",
    "    \"ask_continue\": \"ask_continue\"\n",
    "})\n",
    "\n",
    "graph.add_edge(\"get_reference\", \"ask_continue\")\n",
    "graph.add_conditional_edges(\"ask_continue\", continue_or_not, {\n",
    "    \"ask_question\": \"ask_question\",\n",
    "    END: END\n",
    "})\n",
    "\n",
    "app = graph.compile()\n",
    "app.invoke({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96d7bb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "질문: MES 공정 로그 보관 기간은?\n",
      "\n",
      "답변: 6개월간\n",
      "\n",
      "질문: y\n",
      "\n",
      "답변: 신입 엔지니어는 공정별 교육을\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'y',\n",
       " 'answer': '신입 엔지니어는 공정별 교육을',\n",
       " 'source_docs': ['신입 엔지니어는 공정별 교육을 모두 이수한 후 장비 조작이 가능합니다.',\n",
       "  '신입 엔지니어는 공정별 교육을 모두 이수한 후 장비 조작이 가능합니다.',\n",
       "  '신입 엔지니어는 공정별 교육을 모두 이수한 후 장비 조작이 가능합니다.'],\n",
       " 'show_reference': False,\n",
       " 'continue': False}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")   \n",
    "retriever = Chroma(\n",
    "    persist_directory=\"./chromaDB\",\n",
    "    embedding_function=embedding_model\n",
    ").as_retriever(search_kwargs={\"k\":3})\n",
    "\n",
    "model_id = \"monologg/koelectra-base-v3-finetuned-korquad\"\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(model_id)\n",
    "text_gen = pipeline(\n",
    "    \"question-answering\",\n",
    "    model = qa_model,\n",
    "    tokenizer = qa_tokenizer,\n",
    "    device =-1,\n",
    "    max_length=512,\n",
    "    do_sample=False,\n",
    "    temperature=0.1,\n",
    "    trncation=True\n",
    ")\n",
    "\n",
    "def ask_question(state):\n",
    "    question = input(\"질문을 입력해주세요:\").strip()\n",
    "    docs = retriever.invoke(question)\n",
    "    top_docs = docs[:3]\n",
    "    best_contexts = [doc.page_content for doc in top_docs]\n",
    "    combined_context = \"\\n\".join(best_contexts)\n",
    "    result = text_gen(question=question, context=combined_context)\n",
    "    \n",
    "\n",
    "    res = {\n",
    "        \"question\": question,        \n",
    "        \"answer\": result['answer'],\n",
    "        \"source_docs\": best_contexts\n",
    "    }\n",
    "    return res\n",
    "\n",
    "def get_answer(state):\n",
    "    print(\"\\n질문:\", state[\"question\"])\n",
    "    print(\"\\n답변:\", state[\"answer\"])\n",
    "    return state\n",
    "\n",
    "\n",
    "def ask_reference(state):\n",
    "    reply = input(\"\\n참고 문서를 보겠습니까? (y/n): \").strip()\n",
    "    state[\"show_reference\"] = user_input.startswith((\"예\",\"네\",\"y\"))\n",
    "    return state\n",
    "\n",
    "def get_reference(state):\n",
    "    if state.get(\"show_reference\"):\n",
    "\n",
    "        print(\"\\n[참고 문서]\\n:\", state[\"context\"])\n",
    "    return state\n",
    "\n",
    "\n",
    "def ask_continue(state):\n",
    "    user_input = input(\"\\n추가 질문하시겠습니까? (y/n): \").strip()\n",
    "    state[\"continue\"] = user_input.startswith(\"y\")\n",
    "    return state\n",
    "  \n",
    "def should_show_reference(state):\n",
    "    return \"get_reference\" if state.get(\"show_reference\") else \"ask_continue\"\n",
    "\n",
    "def should_continue(state):\n",
    "    return \"ask_question\" if state.get(\"continue\")  else END\n",
    "\n",
    "\n",
    "\n",
    "graph = StateGraph(dict)\n",
    "graph.add_node(\"ask_question\", ask_question)\n",
    "graph.add_node(\"get_answer\", get_answer)\n",
    "graph.add_node(\"ask_reference\", ask_reference)\n",
    "graph.add_node(\"get_reference\", get_reference)\n",
    "graph.add_node(\"ask_continue\", ask_continue)\n",
    "\n",
    "graph.set_entry_point(\"ask_question\")\n",
    "\n",
    "graph.add_edge(\"ask_question\", \"get_answer\")\n",
    "graph.add_edge(\"get_answer\", \"ask_reference\")\n",
    "graph.add_conditional_edges(\"ask_reference\", should_show_reference, {\n",
    "    \"get_reference\": \"get_reference\",\n",
    "    \"ask_continue\": \"ask_continue\"\n",
    "})\n",
    "\n",
    "graph.add_edge(\"get_reference\", \"ask_continue\")\n",
    "graph.add_conditional_edges(\"ask_continue\", should_continue, {\n",
    "    \"ask_question\": \"ask_question\",\n",
    "    END: END\n",
    "})\n",
    "\n",
    "app = graph.compile()\n",
    "app.invoke({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25762af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "질문: MES 공정 로그 보관 기간은?\n",
      "\n",
      "답변: 6개월간\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'MES 공정 로그 보관 기간은?',\n",
       " 'answer': '6개월간',\n",
       " 'context': ['MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.',\n",
       "  'MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.',\n",
       "  'MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.'],\n",
       " 'show_reference': False,\n",
       " 'continue': False}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "from langchain_chroma import Chroma\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")   \n",
    "retriever = Chroma(\n",
    "    persist_directory=\"./chromaDB\",\n",
    "    embedding_function=embedding_model\n",
    ").as_retriever(search_kwargs={\"k\":3})\n",
    "\n",
    "model_id = \"monologg/koelectra-base-v3-finetuned-korquad\"\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(model_id)\n",
    "text_gen = pipeline(\n",
    "    \"question-answering\",\n",
    "    model = qa_model,\n",
    "    tokenizer = qa_tokenizer,\n",
    "    device =-1,\n",
    "    max_length=512,\n",
    "    do_sample=False,\n",
    "    temperature=0.1,\n",
    "    trncation=True\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=text_gen)\n",
    "chats_by_ssetion_id = {}\n",
    "\n",
    "def get_chat_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in chats_by_ssetion_id:\n",
    "        chats_by_ssetion_id[session_id] = InMemoryChatMessageHistory()\n",
    "    return chats_by_ssetion_id[session_id]\n",
    "\n",
    "def ask_question(state, config:RunnableConfig):\n",
    "    session_id = config[\"configurable\"][\"session_id\"]\n",
    "    chat_history = get_chat_history(session_id)\n",
    "\n",
    "    question = input(\"질문을 입력해주세요:\").strip()\n",
    "    docs = retriever.invoke(question)\n",
    "    top_docs = docs[:3]\n",
    "    best_contexts = [doc.page_content for doc in top_docs]\n",
    "    combined_context = \"\\n\".join(best_contexts)\n",
    "\n",
    "    history_text = get_buffer_string(chat_history.messages)\n",
    "    full_context = f\"{history_text}\\n\\n{combined_context}\" if history_text else combined_context\n",
    "\n",
    "    result = text_gen(question=question, context=full_context)\n",
    "    chat_history.add_user_message(question)\n",
    "    chat_history.add_ai_message(result['answer'])\n",
    "    return {\n",
    "        \"question\": question,        \n",
    "        \"answer\": result['answer'],\n",
    "        \"context\": best_contexts\n",
    "    }\n",
    "\n",
    "\n",
    "def get_answer(state):\n",
    "    print(f\"\\n질문:\", state['question'])\n",
    "    print(f\"\\n답변:\", state['answer'])\n",
    "    return state\n",
    "\n",
    "\n",
    "def ask_reference(state):\n",
    "    reply = input(\"\\n참고 문서를 보겠습니까? (y/n): \").strip()\n",
    "    state[\"show_reference\"] = user_input.startswith(\"y\")\n",
    "    return state\n",
    "\n",
    "def get_reference(state):\n",
    "    if state.get(\"show_reference\"):\n",
    "        print (\"\\n참고문서:\\n\")\n",
    "        for i, ctx in enumerate(state[\"context\"], 1):\n",
    "            print(f\"[{i}] {ctx}\\n\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def ask_continue(state):\n",
    "    user_input = input(\"\\n추가 질문하시겠습니까? (y/n): \").strip()\n",
    "    state[\"continue\"] = user_input.startswith(\"y\")\n",
    "    return state\n",
    "  \n",
    "def should_show_reference(state):\n",
    "    return \"get_reference\" if state.get(\"show_reference\") else \"ask_continue\"\n",
    "\n",
    "def should_continue(state):\n",
    "    return \"ask_question\" if state.get(\"continue\")  else END\n",
    "\n",
    "\n",
    "graph = StateGraph(dict)\n",
    "graph.add_node(\"ask_question\", ask_question)\n",
    "graph.add_node(\"get_answer\", get_answer)\n",
    "graph.add_node(\"ask_reference\", ask_reference)\n",
    "graph.add_node(\"get_reference\", get_reference)\n",
    "graph.add_node(\"ask_continue\", ask_continue)\n",
    "\n",
    "graph.set_entry_point(\"ask_question\")\n",
    "\n",
    "graph.add_edge(\"ask_question\", \"get_answer\")\n",
    "graph.add_edge(\"get_answer\", \"ask_reference\")\n",
    "graph.add_conditional_edges(\"ask_reference\", should_show_reference, {\n",
    "    \"get_reference\": \"get_reference\",\n",
    "    \"ask_continue\": \"ask_continue\"\n",
    "})\n",
    "\n",
    "graph.add_edge(\"get_reference\", \"ask_continue\")\n",
    "graph.add_conditional_edges(\"ask_continue\", should_continue, {\n",
    "    \"ask_question\": \"ask_question\",\n",
    "    END: END\n",
    "})\n",
    "\n",
    "session_id = str(uuid.uuid4())\n",
    "config = {\"configurable\": {\"session_id\": session_id}}\n",
    "app = graph.compile()\n",
    "app.invoke({}, config= config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eb772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word 파일을 읽어서 ChromaDB에 저장하는 예시 함수\n",
    "from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "def save_word_to_chromadb(docx_path, chroma_dir, embedding_model_name):\n",
    "    # 1. Word 파일 읽기\n",
    "    loader = UnstructuredWordDocumentLoader(docx_path)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # 2. 임베딩 모델 준비\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "    \n",
    "    # 3. ChromaDB에 저장\n",
    "    vectorstore = Chroma.from_documents(\n",
    "    \tdocuments=docs,\n",
    "    \tembedding=embedding_model,\n",
    "    \tpersist_directory=chroma_dir\n",
    "    )\n",
    "    print(f\"{len(docs)}개의 문서가 ChromaDB에 저장되었습니다.\")\n",
    "    return vectorstore\n",
    "\n",
    "# 사용 예시\n",
    "chroma_dir = \"./chromaDB\"\n",
    "docx_path = \"NewBie_개발환경가이드1.docx\"\n",
    "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "vectorstore = save_word_to_chromadb(docx_path, chroma_dir, embedding_model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI2503",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
