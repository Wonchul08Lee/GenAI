{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad8dee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "df = pd.read_csv(\"./data/data_topic.csv\", encoding=\"utf8\")\n",
    "df.head(10)\n",
    "\n",
    "texts = df[\"text\"].tolist()\n",
    "topics = df[\"topic\"].tolist()\n",
    "docs = []\n",
    "for i in range(len(texts)):\n",
    "    text = texts[i]\n",
    "    topic = topics[i]\n",
    "    doc = Document(page_content=text, metadata={\"topic\": topic})\n",
    "    docs.append(doc)\n",
    "\n",
    "docs[0:3]\n",
    "\n",
    "embedding_model= HuggingFaceEmbeddings(model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding = embedding_model, \n",
    "    persist_directory=\"./chromaDB_topic\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1c060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForQuestionAnswering, pipeline\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "\n",
    "\n",
    "embdedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraprhrase-multilingual-MiniLM-L12-v2\")\n",
    "model_id = \"monologg/koelecsmall-v2-finetuned-korquad\"\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(model_id)\n",
    "text_gen = pipeline(\n",
    "    \"question-answering\",\n",
    "    model = qa_model,\n",
    "    tokenizer = qa_tokenizer,\n",
    "    device =-1   \n",
    "    max_length=512,\n",
    "    do_sample=False,\n",
    "    temperature=0.1,\n",
    "    truncation=True\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=text_gen)\n",
    "\n",
    "\n",
    "chats_by_ssetion_id = {}\n",
    "\n",
    "def get_chat_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in chats_by_ssetion_id:\n",
    "        chats_by_ssetion_id[session_id] = InMemoryChatMessageHistory()\n",
    "    return chats_by_ssetion_id[session_id]\n",
    "\n",
    "def ask_question(state, config:RunnableConfig):\n",
    "    session_id = config[\"configurable\"][\"session_id\"]\n",
    "    chat_history = get_chat_history(session_id)\n",
    "\n",
    "    question = input(\"질문을 입력하세요: \").strip()\n",
    "    topic = input(\"관련 토픽을 입력해주세요 (공정 유형, 유지보수, 이상대응, 전산관리, 안전규칙):\").strip()\n",
    "\n",
    "    retriver = Chroma(\n",
    "        embedding_function = embdedding_model,\n",
    "        persist_directory=\"./chromaDB_topic\"\n",
    "    ).as_retriever(search_kwargs={\"k\":3, \"filter\": {\"topic\": topic}})\n",
    "\n",
    "    docs = retriver.invoke(question)\n",
    "    top_docs = docs[:3]\n",
    "    best_contexts = [doc.page_content for doc in top_docs]\n",
    "    combined_context = \"\\n\".join(best_contexts)\n",
    "\n",
    "    history_text = get_buffer_string(chat_history.messages)\n",
    "    full_context = f\"{history_text}\\n{combined_context}\" if history_text else combined_context\n",
    "\n",
    "    result = text_gen(question=question, context=full_context)\n",
    "\n",
    "    chat_history.add_user_message(question)\n",
    "    chat_history.add_ai_message(result['answer'])\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": result['answer'],\n",
    "        \n",
    "        \"context\": best_contexts\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI2503",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
