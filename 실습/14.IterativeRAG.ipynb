{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01c1b3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough answer, try again.\n",
      "\n",
      "재질문: original question: 공정 조건 이상 발생 시 어떻게 처리하나요?\n",
      "\\더 구체적으로 묻기 위해 재질문을 만들어주세요!\n",
      "\\더 구체적으로 묻기 위해 재질문 만들기!\n",
      "\\더 구체적으로 질문하기 위해 재질문 만들기!\n",
      "\\더 구체적으로 질문하기 위해 재질문 만들기!\n",
      "\\더 구체적으로 질문하기 위해 재질문 만들기!\n",
      "\\더 구체적으로 질문하기 위해 재질문 만들기!\n",
      "\\더 구체적으로 질문하기 위해서 재질\n",
      "not enough answer, try again.\n",
      "\n",
      "재질문: original question: 공정 조건 이상 발생 시 어떻게 처리하나요?\n",
      "\\더 구체적으로 묻기 위해 재질문을 만들어주세요!\n",
      "\\더 구체적으로 대답하기 위해 재질문 대신 재질문을 만들어주세요!\n",
      "\\더 구체적으로 대답하기 위해 재질문을 만들어주세요!\n",
      "\\더 구체적으로 대답하기 위해 재질문을 만들어주세요!\n",
      "\\더 구체적으로 대답하기 위해 재질문을 만들어주세요!\n",
      "\\더 구체적으로\n",
      "\n",
      "질문: original question: 공정 조건 이상 발생 시 어떻게 처리하나요?\n",
      "\\더 구체적으로 묻기 위해 재질문을 만들어주세요!\n",
      "\\더 구체적으로 대답하기 위해 재질문 대신 재질문을 만들어주세요!\n",
      "\\더 구체적으로 대답하기 위해 재질문을 만들어주세요!\n",
      "\\더 구체적으로 대답하기 위해 재질문을 만들어주세요!\n",
      "\\더 구체적으로 대답하기 위해 재질문을 만들어주세요!\n",
      "\\더 구체적으로\n",
      "답변:검사 장비는 매일 초기화 후 기능 점검을\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'original question: 공정 조건 이상 발생 시 어떻게 처리하나요?\\n\\\\더 구체적으로 묻기 위해 재질문을 만들어주세요!\\n\\\\더 구체적으로 대답하기 위해 재질문 대신 재질문을 만들어주세요!\\n\\\\더 구체적으로 대답하기 위해 재질문을 만들어주세요!\\n\\\\더 구체적으로 대답하기 위해 재질문을 만들어주세요!\\n\\\\더 구체적으로 대답하기 위해 재질문을 만들어주세요!\\n\\\\더 구체적으로',\n",
       " 'original_question': '공정 조건 이상 발생 시 어떻게 처리하나요?',\n",
       " 'attempt': 3,\n",
       " 'history': InMemoryChatMessageHistory(messages=[HumanMessage(content='공정 조건 이상 발생 시 어떻게 처리하나요?', additional_kwargs={}, response_metadata={}), AIMessage(content='검사 장비는 매일 초기화 후 기능 점검을', additional_kwargs={}, response_metadata={})]),\n",
       " 'docs': [Document(id='8bde1239-dfad-4a24-a67e-d48a3c423905', metadata={}, page_content='검사 장비는 매일 초기화 후 기능 점검을 실시합니다.'),\n",
       "  Document(id='24d1021a-8038-4944-81c7-15744521f2ec', metadata={}, page_content='검사 장비는 매일 초기화 후 기능 점검을 실시합니다.'),\n",
       "  Document(id='93fa7d22-941d-426b-a69d-7779fce64e7b', metadata={}, page_content='검사 장비는 매일 초기화 후 기능 점검을 실시합니다.')],\n",
       " 'context': '검사 장비는 매일 초기화 후 기능 점검을 실시합니다.\\n검사 장비는 매일 초기화 후 기능 점검을 실시합니다.\\n검사 장비는 매일 초기화 후 기능 점검을 실시합니다.',\n",
       " 'answer': '검사 장비는 매일 초기화 후 기능 점검을',\n",
       " 'needs_followup': False,\n",
       " 'continue': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForQuestionAnswering, pipeline\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "\n",
    "\n",
    "df = pd.read_csv('C:\\\\Users\\\\Leo\\\\code\\\\ch07_트랜스포머_RAG\\\\test03\\\\data\\\\data.csv', encoding='utf8')\n",
    "texts = df[\"text\"].tolist()\n",
    "docs = [Document(page_content=text) for text in texts]\n",
    "\n",
    "embdedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=docs, embedding=embdedding_model, \n",
    "    persist_directory=\"./chroma_db3\")\n",
    "\n",
    "retriever = Chroma(\n",
    "    persist_directory=\"./chroma_db3\",\n",
    "    embedding_function=embdedding_model\n",
    ").as_retriever(search_kwargs={\"k\":3})\n",
    "\n",
    "model_id = \"monologg/koelectra-base-v3-finetuned-korquad\"\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(model_id)\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model = qa_model,\n",
    "    tokenizer = qa_tokenizer,\n",
    "    device =-1   \n",
    ")\n",
    "qa_llm = HuggingFacePipeline(pipeline=qa_pipeline)\n",
    "\n",
    "rewrite_model_id = \"skt/kogpt2-base-v2\"\n",
    "rewrite_tokenizer = AutoTokenizer.from_pretrained(rewrite_model_id)\n",
    "rewrite_model = AutoModelForCausalLM.from_pretrained(rewrite_model_id)\n",
    "rewrite_tokenizer.model_max_length = 1024\n",
    "\n",
    "rewrite_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = rewrite_model,\n",
    "    tokenizer = rewrite_tokenizer,\n",
    "    max_new_tokens = 64,\n",
    ")\n",
    "\n",
    "rewrite_llm = HuggingFacePipeline(pipeline=rewrite_pipeline)\n",
    "chats_by_ssetion_id = {}\n",
    "\n",
    "def get_chat_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in chats_by_ssetion_id:\n",
    "        chats_by_ssetion_id[session_id] = InMemoryChatMessageHistory()\n",
    "    return chats_by_ssetion_id[session_id]\n",
    "\n",
    "\n",
    "def ask_question(state, config:RunnableConfig):\n",
    "    session_id = config[\"configurable\"][\"session_id\"]\n",
    "    history = get_chat_history(session_id)\n",
    "\n",
    "    question = input(\"질문을 입력해주세요:\").strip()\n",
    "    history.add_user_message(question)\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,        \n",
    "        \"original_question\": question,\n",
    "        \"attempt\":1,\n",
    "        \"history\": history\n",
    "    }\n",
    "\n",
    "def search_and_answer(state):\n",
    "    question = state[\"question\"]\n",
    "    docs = retriever.invoke(question)\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    result = qa_pipeline(\n",
    "        question=question, context=context)\n",
    "    return {**state, \"docs\": docs, \"context\": context,\"answer\": result[\"answer\"]}\n",
    "\n",
    "def assess_answer_quality(state):\n",
    "    answer = state[\"answer\"].strip()\n",
    "    attempt = state.get(\"attempt\", 1)\n",
    "    if len(answer) < 10 and attempt <3:\n",
    "        print(\"not enough answer, try again.\")\n",
    "        return {**state, \"needs_followup\": True}\n",
    "    print(f\"\\n질문: {state['question']}\\n답변:{answer}\")\n",
    "    return {**state, \"needs_followup\": False}\n",
    "\n",
    "def generate_followup(state):\n",
    "    original_question = state[\"original_question\"]\n",
    "    prompt = f\"original question: {original_question}\\n\\더 구체적으로 묻기 위해 재질문을 만들어주세요\"\n",
    "    followup = rewrite_pipeline(prompt)[0][\"generated_text\"]\n",
    "    print(f\"\\n재질문: {followup.strip()}\")\n",
    "    return {\n",
    "        **state,\n",
    "        \"question\": followup.strip(),\n",
    "        \"attempt\": state[\"attempt\"] + 1\n",
    "    }\n",
    "\n",
    "def finalize(state):\n",
    "    session_id = state.get(\"session_id\")\n",
    "    state[\"history\"].add_ai_message(state[\"answer\"])\n",
    "    return state\n",
    "\n",
    "def ask_continue(state):\n",
    "    cont = input(\"\\n계속하시겠습니까? (y/n): \").strip()\n",
    "    state[\"continue\"] = cont.startswith(\"y\")\n",
    "    return state\n",
    "\n",
    "def should_followup(state):\n",
    "    return \"generate_followup\" if state.get(\"needs_followup\") else \"finalize\"\n",
    "\n",
    "def should_continue(state):\n",
    "    return \"ask_question\" if state.get(\"continue\")  else END\n",
    "\n",
    "\n",
    "graph = StateGraph(dict)\n",
    "graph.add_node(\"ask_question\", ask_question)\n",
    "graph.add_node(\"search_and_answer\", search_and_answer)\n",
    "graph.add_node(\"assess_answer_quality\", assess_answer_quality)\n",
    "graph.add_node(\"generate_followup\", generate_followup)\n",
    "graph.add_node(\"finalize\", finalize)\n",
    "graph.add_node(\"ask_continue\", ask_continue)\n",
    "\n",
    "graph.set_entry_point(\"ask_question\")\n",
    "\n",
    "graph.add_edge(\"ask_question\", \"search_and_answer\")\n",
    "graph.add_edge(\"search_and_answer\", \"assess_answer_quality\")\n",
    "graph.add_conditional_edges(\"assess_answer_quality\", should_followup, {\n",
    "    \"generate_followup\": \"generate_followup\",\n",
    "    \"finalize\": \"finalize\"\n",
    "})\n",
    "\n",
    "graph.add_edge(\"generate_followup\", \"search_and_answer\")\n",
    "graph.add_edge(\"finalize\", \"ask_continue\")\n",
    "graph.add_conditional_edges(\"ask_continue\", should_continue, {\n",
    "    \"ask_question\": \"ask_question\",\n",
    "    END: END\n",
    "})\n",
    "\n",
    "session_id = str(uuid.uuid4())\n",
    "config = {\"configurable\": {\"session_id\": session_id}}\n",
    "app = graph.compile()\n",
    "app.invoke({}, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5144a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI2503",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
