{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45490a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [Agent answering...]\n",
      "Answer: 6개월간\n",
      "\n",
      " [References:]\n",
      "- MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.\n",
      "- MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.\n",
      "- MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mcp': {'source': 'rag_agent',\n",
       "  'destination': 'supervisor',\n",
       "  'intent': 'answer',\n",
       "  'payload': {'answer': '6개월간',\n",
       "   'references': ['MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.',\n",
       "    'MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.',\n",
       "    'MES 시스템에 기록된 공정 로그는 6개월간 보관됩니다.'],\n",
       "   'metadata': {'session_id': '6396df29-9b7f-4a01-a74b-00de82e44a4c'}}},\n",
       " 'continue': False}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoModelForQuestionAnswering, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    pipeline)\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./data/data.csv\", encoding=\"utf8\")\n",
    "texts = df['text'].tolist()\n",
    "docs = [Document(page_content=text) for text in texts]\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./MultiAgentChroma_db\",\n",
    ")\n",
    "\n",
    "\n",
    "chats_by_ssetion_id = {}\n",
    "\n",
    "def get_chat_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in chats_by_ssetion_id:\n",
    "        chats_by_ssetion_id[session_id] = InMemoryChatMessageHistory()\n",
    "    return chats_by_ssetion_id[session_id]\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "retriever = Chroma (\n",
    "    persist_directory=\"./MultiAgentChroma_db\",\n",
    "    embedding_function=embedding_model\n",
    ").as_retriever(search_kwargs={\"k\":3})\n",
    "\n",
    "qa_model_id = \"monologg/koelectra-base-v3-finetuned-korquad\"\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_id)\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_id)\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=qa_model,\n",
    "    tokenizer=qa_tokenizer,\n",
    "    device=-1\n",
    ")\n",
    "qa_llim = HuggingFacePipeline(pipeline=qa_pipeline)\n",
    "\n",
    "sum_model_id = \"lcw99/t5-base-korean-text-summary\"\n",
    "sum_tokenizer = AutoTokenizer.from_pretrained(sum_model_id)\n",
    "sum_model = AutoModelForSeq2SeqLM.from_pretrained(sum_model_id)\n",
    "sum_pipeline = pipeline(\n",
    "    \"summarization\",\n",
    "    model=sum_model,\n",
    "    tokenizer=sum_tokenizer,\n",
    "    device=-1,\n",
    ")\n",
    "sum_llim = HuggingFacePipeline(pipeline=sum_pipeline)\n",
    "\n",
    "gen_model_id = \"skt/kogpt2-base-v2\"\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_id)\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(gen_model_id)\n",
    "gen_tokenizer.model_max_length = 1024\n",
    "gen_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=gen_model,\n",
    "    tokenizer=gen_tokenizer,\n",
    "    device=-1,\n",
    "    max_new_tokens=300,\n",
    "    do_sample=True)\n",
    "\n",
    "gen_llim = HuggingFacePipeline(pipeline=gen_pipeline)\n",
    "\n",
    "def rag_node(state: Dict[str, Any]):\n",
    "    mcp = state[\"mcp\"]\n",
    "    payload = mcp[\"payload\"]\n",
    "    question = payload[\"question\"]\n",
    "    sesseion_id = payload[\"metadata\"][\"session_id\"]\n",
    "\n",
    "    chat_history = get_chat_history(sesseion_id)\n",
    "    history_text = \"\\n\".join([m.content for m in chat_history.messages])\n",
    "    docs = retriever.invoke(question)\n",
    "    top_docs = docs[:3]\n",
    "    context = \"\\n\".join([doc.page_content for doc in top_docs])\n",
    "    full_context = f\"{history_text}\\n{context}\" if history_text else context\n",
    "    result = qa_pipeline(question=question, context=full_context)\n",
    "\n",
    "\n",
    "    chat_history.add_user_message(question)\n",
    "    chat_history.add_ai_message(result['answer'])\n",
    "\n",
    "    return {\n",
    "        \"mcp\": {\n",
    "            \"source\": \"rag_agent\",\n",
    "            \"destination\": mcp[\"source\"],\n",
    "            \"intent\":\"answer\",\n",
    "            \"payload\": {\n",
    "                \"answer\": result['answer'],\n",
    "                \"references\": [doc.page_content for doc in top_docs],\n",
    "                \"metadata\": {\n",
    "                    \"session_id\": sesseion_id\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "def build_rag_agent():\n",
    "    graph = StateGraph(dict)\n",
    "    graph.set_entry_point(\"rag_node\")\n",
    "    graph.add_node(\"rag_node\", rag_node)\n",
    "    graph.set_finish_point(\"rag_node\")\n",
    "    return graph.compile()\n",
    "\n",
    "rag_app = build_rag_agent()\n",
    "\n",
    "def summarize_node(state: Dict[str, Any]):\n",
    "    mcp = state[\"mcp\"]\n",
    "    payload = mcp[\"payload\"]\n",
    "    text = payload[\"question\"]\n",
    "    sesseion_id = payload[\"metadata\"][\"session_id\"]\n",
    "    result = sum_llim.invoke(text)\n",
    "    \n",
    "    return {\n",
    "        \"mcp\": {\n",
    "            \"source\": \"summarize_agent\",\n",
    "            \"destination\": mcp[\"source\"],\n",
    "            \"intent\":\"answer\",\n",
    "            \"payload\": {\n",
    "                \"answer\": result,\n",
    "                \"metadata\": {\n",
    "                    \"session_id\": sesseion_id\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "def build_summarize_agent():\n",
    "    graph = StateGraph(dict)\n",
    "    graph.set_entry_point(\"summarize_node\")\n",
    "    graph.add_node(\"summarize_node\", summarize_node)\n",
    "    graph.set_finish_point(\"summarize_node\")\n",
    "    return graph.compile()\n",
    "\n",
    "summarize_app = build_summarize_agent()\n",
    "\n",
    "def rephrase_node(state: Dict[str, Any]):\n",
    "    mcp = state[\"mcp\"]\n",
    "    payload = mcp[\"payload\"]\n",
    "    question = payload[\"question\"]\n",
    "    sesseion_id = payload[\"metadata\"][\"session_id\"]\n",
    "    result = gen_llim.invoke(question)\n",
    "    \n",
    "    return {\n",
    "        \"mcp\": {\n",
    "            \"source\": \"rephrase_agent\",\n",
    "            \"destination\": mcp[\"source\"],\n",
    "            \"intent\":\"answer\",\n",
    "            \"payload\": {\n",
    "                \"answer\": result,\n",
    "                \"metadata\": {\n",
    "                    \"session_id\": sesseion_id\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "def build_rephrase_agent():\n",
    "    graph = StateGraph(dict)\n",
    "    graph.set_entry_point(\"rephrase_node\")\n",
    "    graph.add_node(\"rephrase_node\", rephrase_node)\n",
    "    graph.set_finish_point(\"rephrase_node\")\n",
    "    return graph.compile()\n",
    "rephrase_app = build_rephrase_agent()\n",
    "\n",
    "def classify_intent(question:str) -> str:\n",
    "    if \"요약\" in question:\n",
    "        return \"summarize\"\n",
    "    elif \"정중\" in question or \"공손\" in question or \"예의\" in question:\n",
    "        return \"rephrase\"\n",
    "    else:\n",
    "        return \"get_answer\"\n",
    "    \n",
    "def supervisor_node(state: Dict[str, Any]):\n",
    "    question = input(\"질문을 입력해라:\").strip()\n",
    "\n",
    "    sesseion_id = state.get(\"session_id\", str(uuid.uuid4()))\n",
    "    intent = classify_intent(question)\n",
    "\n",
    "    dest_map = {\n",
    "        \"get_answer\": \"rag_agent\",\n",
    "        \"summarize\": \"summarize_agent\",\n",
    "        \"rephrase\": \"rephrase_agent\"\n",
    "    }\n",
    "    mcp = {\n",
    "        \"source\": \"supervisor\",\n",
    "        \"destination\": dest_map[intent],\n",
    "        \"intent\":\"request\",\n",
    "        \"payload\": {\n",
    "            \"question\": question,\n",
    "            \"metadata\": {\n",
    "                \"session_id\": sesseion_id\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return {\"mcp\": mcp, \"session_id\": sesseion_id}\n",
    "\n",
    "def get_answer(state: Dict[str, Any]):\n",
    "    mcp = state[\"mcp\"]\n",
    "    print(\"\\n [Agent answering...]\")\n",
    "    print(\"Answer:\", mcp[\"payload\"][\"answer\"])\n",
    "\n",
    "    if \"references\" in mcp[\"payload\"]:\n",
    "        print(\"\\n [References:]\")\n",
    "        for ref in mcp[\"payload\"][\"references\"]:\n",
    "            print(\"-\", ref)\n",
    "                  \n",
    "    return state\n",
    "\n",
    "def route_mcp(state):\n",
    "    return state[\"mcp\"][\"destination\"]\n",
    "def ask_continue(state: Dict[str, Any]):\n",
    "    user_input = input(\"\\n계속 질문하시겠습니까? (y/n): \").strip()\n",
    "    state[\"continue\"] = user_input.lower().startswith('y')\n",
    "    return state\n",
    "def should_continue(state: Dict[str, Any]):\n",
    "    return \"supervisor\" if state.get(\"continue\") else END\n",
    "\n",
    "def build_supervisor_graph():\n",
    "    graph = StateGraph(dict)\n",
    "    graph.set_entry_point(\"supervisor_node\")\n",
    "    graph.add_node(\"supervisor_node\", supervisor_node)\n",
    "    \n",
    "    graph.add_node(\"rag_agent\", lambda state:rag_app.invoke(state))\n",
    "    graph.add_node(\"summarize_agent\", lambda state:summarize_app.invoke(state))\n",
    "    graph.add_node(\"rephrase_agent\", lambda state:rephrase_app.invoke(state))\n",
    "    \n",
    "    graph.add_node(\"get_answer\", get_answer)\n",
    "    graph.add_node(\"ask_continue\", ask_continue)\n",
    "\n",
    "    graph.add_conditional_edges(\"supervisor_node\", route_mcp, {\n",
    "        \"rag_agent\": \"rag_agent\",\n",
    "        \"summarize_agent\": \"summarize_agent\",\n",
    "        \"rephrase_agent\": \"rephrase_agent\"\n",
    "    })\n",
    "    graph.add_edge(\"rag_agent\", \"get_answer\")\n",
    "    graph.add_edge(\"summarize_agent\", \"get_answer\")\n",
    "    graph.add_edge(\"rephrase_agent\", \"get_answer\")\n",
    "    graph.add_edge(\"get_answer\", \"ask_continue\")\n",
    "\n",
    "    graph.add_conditional_edges(\"ask_continue\", should_continue, {\n",
    "        \"supervisor\": \"supervisor_node\",\n",
    "        END: END\n",
    "    })\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "supervisor_app = build_supervisor_graph()\n",
    "\n",
    "supervisor_app.invoke({})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI2503",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
