{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b55f2a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final Answer:\n",
      "다음 내용을 자연스럽게 다시 표현해줘:\n",
      "6개월간 진행된 연구에서, 실험 참가자들은 이 같은 내용을 기억하지 못했다.\n",
      "실험 참가자들은 실험 동안 자신이 실제로 받은 감정 중 어떤 것이 실제로 어떤 결과를 가져오는지를 떠올렸다.\n",
      "실험 후 참가자들은 실험 결과에 대한 기억이 없다는 사실을 발견했다.\n",
      "그들은 실험 참가자들이 기억하지 못하는 감정들을 측정했다.\n",
      "그 결과, 실험 참가자들이 경험한 감정을 기억하지 못하는 것으로 나타났다.\n",
      "또한 실험 참가자들은 실험 참가자들이 실험에서 사용한 감정 중 어떤 것이 실제로 어떤 결과를 가져오는지 생각해보지도 않았다.\n",
      "그 결과 실험 참가자들은 실험 참여자들이 실제 경험했던 감정 중 어떤 것이 실제로 어떠한 결과를 가져오는지 생각해보지도 못했다.\n",
      "또한 실험 참가자들에게 실험 참가자들은 실험이 어떤 결과를 가져오는지 상상해보기도 하지 않았다.\n",
      "즉, 실험 참가자들은 실험 참가자들이 경험한 감정들을 기억해내지 못했다.\n",
      "그러나 실험 참가자들은 실제 경험하지 못하는 감정들을 상상해보지도 않았다.\n",
      "그들은 실험 참가자들이 경험한 감정들을 기억하지 못했다.\n",
      "즉, 실험 참가자들은 실제 경험하지 못하는 감정들을 상상해보기도 않았다.\n",
      "이 같은 결과는 실험 참가자들이 실제 경험하지 못하는 감정들을 상상해보지 않았기 때문이다.\n",
      "그렇다면 실험 참가자들은 실제로 경험하지 못하는 감정들을 상상해보기도 하지 않은 것일까?\n",
      "이번 연구는 실험 참가자들이 실험 참가자들에게 어떤 감정을 기억하는지 알 수 있도록 해준다는 데에 의미가 있다.\n",
      "즉, 실험 참가자들은 실험 참가자들이 실제 경험하지 못하는 감정들을 상상해보기도 하지 않은 채 실험 참가자들이 경험하지 못하는 감정들만을 상상해보는 것이 아니라, 실험 참가자들이 실험 참가자들이 경험하지 못하는 감정들을 상상해본다는\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoModelForQuestionAnswering, \n",
    "    pipeline)\n",
    "from typing import Dict, Any\n",
    "\n",
    "chats_by_ssetion_id = {}\n",
    "\n",
    "def get_chat_history(session_id: str):\n",
    "    if session_id not in chats_by_ssetion_id:\n",
    "        chats_by_ssetion_id[session_id] = InMemoryChatMessageHistory()\n",
    "    return chats_by_ssetion_id[session_id]\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "retriever = Chroma (\n",
    "    persist_directory=\"./MultiAgentChroma_db\",\n",
    "    embedding_function=embedding_model\n",
    ").as_retriever(search_kwargs={\"k\":3})\n",
    "\n",
    "# KoElectra QA 모델\n",
    "qa_model_id = \"monologg/koelectra-base-v3-finetuned-korquad\"\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_id)\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_id)\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=qa_model,\n",
    "    tokenizer=qa_tokenizer,\n",
    "    device=-1\n",
    ")\n",
    "qa_llim = HuggingFacePipeline(pipeline=qa_pipeline)\n",
    "\n",
    "\n",
    "gen_model_id = \"skt/kogpt2-base-v2\"\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_id)\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(gen_model_id)\n",
    "gen_tokenizer.model_max_length = 1024\n",
    "gen_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=gen_model,\n",
    "    tokenizer=gen_tokenizer,\n",
    "    device=-1,\n",
    "    max_new_tokens=300,\n",
    "    do_sample=True)\n",
    "\n",
    "gen_llm = HuggingFacePipeline(pipeline=gen_pipeline)\n",
    "\n",
    "\n",
    "def rag_node(state: Dict[str, Any]):\n",
    "    mcp = state[\"mcp\"]\n",
    "    payload = mcp[\"payload\"]\n",
    "    question = payload[\"question\"]\n",
    "    sesseion_id = payload[\"metadata\"][\"session_id\"]\n",
    "\n",
    "    chat_history = get_chat_history(sesseion_id)\n",
    "    history_text = \"\\n\".join([m.content for m in chat_history.messages])\n",
    "    docs = retriever.invoke(question)\n",
    "    top_docs = docs[:3]\n",
    "    context = \"\\n\".join([doc.page_content for doc in top_docs])\n",
    "    full_context = f\"{history_text}\\n{context}\" if history_text else context\n",
    "\n",
    "    result = qa_pipeline(question=question, context=full_context)\n",
    "    answer = result['answer']\n",
    "    chat_history.add_user_message(question)\n",
    "    chat_history.add_ai_message(result['answer'])\n",
    "\n",
    "    return {\n",
    "        \"mcp\": {\n",
    "            \"source\": \"rag_agent\",\n",
    "            \"destination\": \"textgent_agent\",\n",
    "            \"intent\":\"rephrase\",\n",
    "            \"payload\": {\n",
    "                \"base_answer\":answer,\n",
    "                \"metadata\": {\n",
    "                    \"session_id\": sesseion_id\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "def build_rag_agent():\n",
    "    graph = StateGraph(dict)\n",
    "    graph.set_entry_point(\"rag_node\")\n",
    "    graph.add_node(\"rag_node\", rag_node)\n",
    "    graph.set_finish_point(\"rag_node\")\n",
    "    return graph.compile()\n",
    "\n",
    "rag_app = build_rag_agent()\n",
    "\n",
    "def textgent_node(state: Dict[str, Any]):\n",
    "    mcp = state[\"mcp\"]\n",
    "    \n",
    "    answer= mcp[\"payload\"][\"base_answer\"]\n",
    "    sesseion_id = mcp[\"payload\"][\"metadata\"][\"session_id\"]\n",
    "\n",
    "    prompt = f\"다음 내용을 자연스럽게 다시 표현해줘:\\n{answer}\"\n",
    "    result = gen_llm.invoke(prompt)\n",
    "\n",
    "    return {\n",
    "        \"mcp\": {\n",
    "            \"source\": \"textgent_agent\",\n",
    "            \"destination\": \"supervisor\",\n",
    "            \"intent\":\"final_answer\",\n",
    "            \"payload\": {\n",
    "                \"final_answer\": result,\n",
    "                \"metadata\": {\n",
    "                    \"session_id\": sesseion_id\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "def build_textgent_agent():\n",
    "    graph = StateGraph(dict)\n",
    "    graph.set_entry_point(\"textgent_node\")\n",
    "    graph.add_node(\"textgent_node\", textgent_node)\n",
    "    graph.set_finish_point(\"textgent_node\")\n",
    "    return graph.compile()\n",
    "\n",
    "textgent_app = build_textgent_agent()\n",
    "def handle_mcp(mcp: Dict[str, Any]):\n",
    "    if mcp[\"destination\"] == \"rag_agent\":\n",
    "        return rag_app.invoke({\"mcp\": mcp})\n",
    "    elif mcp[\"destination\"] == \"textgent_agent\":\n",
    "        return textgent_app.invoke({\"mcp\": mcp})\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown destination: {mcp['destination']}\")\n",
    "    \n",
    "session_id = str(uuid.uuid4())\n",
    "\n",
    "while True:\n",
    "    question = input(\"\\n input question:\").strip()\n",
    "    if not question:\n",
    "        break\n",
    "\n",
    "    mcp = {\n",
    "        \"source\":\"supervisor\",\n",
    "        \"destination\":\"rag_agent\",\n",
    "        \"intent\":\"get_answer\",\n",
    "        \"payload\":{\n",
    "            \"question\": question,\n",
    "            \"metadata\":{\n",
    "                \"session_id\": session_id\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    result1 = handle_mcp(mcp)\n",
    "    result2 = handle_mcp(result1[\"mcp\"])\n",
    "\n",
    "    final_answer = result2[\"mcp\"][\"payload\"][\"final_answer\"]\n",
    "    print(\"\\n Final Answer:\")\n",
    "    print(final_answer)\n",
    "\n",
    "    cont = input(\"\\nContinue? (y/n):\").strip()\n",
    "    if not cont.startswith('y'):\n",
    "        break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI2503",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
