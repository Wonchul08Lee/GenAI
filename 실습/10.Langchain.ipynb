{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "667697c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "산 속에 토끼 한 마리가 살고 있었습니다. 그러던 어느 날 토끼가 토끼와 함께 마을로 들어왔습니다. 토끼는 그곳에서 토끼를 잡아먹은 뒤 마을을 떠나는 꿈을 꿨습니다. 그런데 토끼는 마을로 돌아온 뒤 토끼를 붙잡아 마을 밖으로 내쫓고 말았습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_id = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "text_gen = pipeline(\n",
    "    task= \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=100,\n",
    "    truncation=True,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    device = -1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_gen)\n",
    "prompt = \"산 속에 토끼 한 마리가 살고 있었습니다. 그러던 어느 날\"\n",
    "response = llm.invoke(prompt)\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819bbb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "산 속에 토끼 한 마리가 살고 있었습니다. 그러던 어느 날 토끼를 발견했습니다. 토끼는 '저 괴물이 이쪽 저쪽 저쪽 저쪽을 왔다 갔다 하는 줄 알고 저를 찾아왔어요. 그래서 그 괴물이 저쪽\n"
     ]
    }
   ],
   "source": [
    "# langchain 만 사용\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "model_id = \"skt/kogpt2-base-v2\"\n",
    "model_opt = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "pipeline_opt = {\n",
    "    \"max_length\": 100,\n",
    "    \"truncation\": True,\n",
    "}\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    task = \"text-generation\",\n",
    "    model_id=model_id,\n",
    "    model_kwargs=model_opt,\n",
    "    pipeline_kwargs=pipeline_opt,\n",
    "    device = -1\n",
    ")\n",
    "\n",
    "prompt = \"산 속에 토끼 한 마리가 살고 있었습니다. 그러던 어느 날\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2e57259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한글을 창제하고 과학, 음악, 농업\n"
     ]
    }
   ],
   "source": [
    "#[Q&A]\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "model_id = \"monologg/koelectra-base-v3-finetuned-korquad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer_opt = {\n",
    "    \"max_length\": 512,\n",
    "    \"truncation\": True,\n",
    "}\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_id)\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    task=\"question-answering\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    tokenizer_kwargs=tokenizer_opt,\n",
    "    device=-1\n",
    ")\n",
    "question = \"세종대왕은 어떤 업적을 남겼나요?\"\n",
    "context = (\n",
    "    \"세종대왕은 조선의 네 번쨰 왕으로, 한글을 창제하고 과학, 음악, 농업 등 다양한 분야에서 업적을 남겼습니다.\"\n",
    "    \"그능 집현저을 설치하고 학문을 장려하였으며, 측우기와 해시계 등의 과학 기구 개발을 지원했습니다.\"\n",
    ")\n",
    "\n",
    "response = qa_pipeline(question=question, context=context)\n",
    "print(response['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3f35741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "산 속에 토끼 한 마리가 살고 있었습니다. 그러던 어느 날 토끼가 나타나서 토끼에게 다가가서 꼼짝도 하지 않고 있습니다. 그런데 그 토끼가 어딘가로 사라져버렸습니다. 어딘가로 사라져버린 토끼는 왜 토끼를 찾아왔을\n"
     ]
    }
   ],
   "source": [
    "# prompt template 사용\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "model_id = \"skt/kogpt2-base-v2\"\n",
    "model_opt = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "pipeline_opt = {\n",
    "    \"max_length\": 100,\n",
    "    \"truncation\": True,\n",
    "}\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    task = \"text-generation\",\n",
    "    model_id=model_id,\n",
    "    model_kwargs=model_opt,\n",
    "    pipeline_kwargs=pipeline_opt,\n",
    "    device = -1\n",
    ")\n",
    "\n",
    "template = \"산 속에 {animal} 한 마리가 살고 있었습니다. 그러던 어느 날\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({\"animal\": \"토끼\"})\n",
    "print(response)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI2503",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
