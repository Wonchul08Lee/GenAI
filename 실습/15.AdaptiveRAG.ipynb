{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e54d2c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Simple RAG] Question: MES 공정 로그 보관 기간은?\n",
      "답변: 6개월간\n",
      "\n",
      "[Iterative RAG] Question: 해당 기간이 지나면 어떻게 되나?\n",
      "답변: 경로 장애 감지 시 자동 우회합니다\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': '해당 기간이 지나면 어떻게 되나?',\n",
       " 'history': InMemoryChatMessageHistory(messages=[HumanMessage(content='MES 공정 로그 보관 기간은?', additional_kwargs={}, response_metadata={}), AIMessage(content='6개월간', additional_kwargs={}, response_metadata={}), HumanMessage(content='해당 기간이 지나면 어떻게 되나?', additional_kwargs={}, response_metadata={}), AIMessage(content='경로 장애 감지 시 자동 우회합니다', additional_kwargs={}, response_metadata={})]),\n",
       " 'strategy': 'iterative',\n",
       " 'attempt': 1,\n",
       " 'answer': '경로 장애 감지 시 자동 우회합니다',\n",
       " 'continue': False}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForQuestionAnswering, pipeline\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "\n",
    "\n",
    "df = pd.read_csv('./data/data.csv', encoding='utf8')\n",
    "\n",
    "texts = df[\"text\"].tolist()\n",
    "docs = [Document(page_content=text) for text in texts]\n",
    "\n",
    "embdedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=docs, embedding=embdedding_model, \n",
    "    persist_directory=\"./chroma_AdaptiveRAGDB\")\n",
    "\n",
    "retriever = Chroma(\n",
    "    persist_directory=\"./chroma_AdaptiveRAGDB\",\n",
    "    embedding_function=embdedding_model\n",
    ").as_retriever(search_kwargs={\"k\":3})\n",
    "\n",
    "model_id = \"monologg/koelectra-base-v3-finetuned-korquad\"\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(model_id)\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model = qa_model,\n",
    "    tokenizer = qa_tokenizer,\n",
    "    device =-1   \n",
    ")\n",
    "qa_llm = HuggingFacePipeline(pipeline=qa_pipeline)\n",
    "\n",
    "\n",
    "rewrite_model_id = \"skt/kogpt2-base-v2\"\n",
    "rewrite_tokenizer = AutoTokenizer.from_pretrained(rewrite_model_id)\n",
    "rewrite_model = AutoModelForCausalLM.from_pretrained(rewrite_model_id)\n",
    "rewrite_tokenizer.model_max_length = 1024\n",
    "\n",
    "rewrite_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = rewrite_model,\n",
    "    tokenizer = rewrite_tokenizer,\n",
    "    max_new_tokens = 64,\n",
    ")\n",
    "rewrite_llm = HuggingFacePipeline(pipeline=rewrite_pipeline)\n",
    "\n",
    "chats_by_ssetion_id = {}\n",
    "\n",
    "def get_chat_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in chats_by_ssetion_id:\n",
    "        chats_by_ssetion_id[session_id] = InMemoryChatMessageHistory()\n",
    "    return chats_by_ssetion_id[session_id]\n",
    "\n",
    "def ask_question(state, config:RunnableConfig):\n",
    "    session_id = config[\"configurable\"][\"session_id\"]\n",
    "    chat_history = get_chat_history(session_id)\n",
    "\n",
    "    question = input(\"질문을 입력해주세요:\").strip()\n",
    "    chat_history.add_user_message(question)\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,        \n",
    "        \"history\": chat_history\n",
    "    }\n",
    "\n",
    "def classify_question(state):\n",
    "    question = state[\"question\"]\n",
    "    if len(question) < 15:\n",
    "        strategy = \"simple\"\n",
    "    elif any (kw in question for kw in [\"어떻게\", \"왜\", \"조건\", \"경우\"]):\n",
    "        strategy = \"iterative\"\n",
    "    else:\n",
    "        strategy = \"default\"\n",
    "    return {**state, \"strategy\": strategy, \"attempt\": 1}\n",
    "\n",
    "def rag_simple(state):\n",
    "    docs = retriever.invoke(state[\"question\"])\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    result = qa_pipeline(\n",
    "        question=state[\"question\"], context=context)    \n",
    "    print(f\"[Simple RAG] Question: {state['question']}\\n답변: {result['answer']}\\n\")\n",
    "    state[\"history\"].add_ai_message(result['answer'])\n",
    "    return {**state, \"answer\": result[\"answer\"]}\n",
    "\n",
    "def rag_iterative(state):\n",
    "    if state[\"attempt\"] > 2:\n",
    "        print(\"\\n[Iterateive RAG] 최대 재질문 횟수 도달, 종료합니다.\")\n",
    "        return state\n",
    "    \n",
    "    docs = retriever.invoke(state[\"question\"])\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    result = qa_pipeline(\n",
    "        question=state[\"question\"], context=context)\n",
    "    \n",
    "    answer = result[\"answer\"]\n",
    "    if len(answer.strip()) < 10:\n",
    "        print(\"\\n[Iterative RAG] 불충분하 답변, 재질문 시도 중...\")\n",
    "        prompt = f\"다음 질문을 더 구체적으로 바꿔주세요: {state['question']}\"\n",
    "        followup = rewrite_pipeline(prompt)[0]['generated_text']\n",
    "        print(f\"[Iterative RAG] 재질문: {followup.strip()}\\n\")\n",
    "\n",
    "        return {**state, \"question\": followup.strip(), \"attempt\": state[\"attempt\"] + 1}\n",
    "    \n",
    "    print(f\"[Iterative RAG] Question: {state['question']}\\n답변: {answer}\\n\")\n",
    "    state[\"history\"].add_ai_message(answer)\n",
    "    return {**state, \"answer\": answer}\n",
    "\n",
    "def route_strategy(stage):\n",
    "    strategy = stage[\"strategy\"]\n",
    "    if strategy == \"simple\":\n",
    "        return \"rag_simple\"\n",
    "    elif strategy == \"iterative\":\n",
    "        return \"rag_iterative\"\n",
    "    else:\n",
    "        return \"rag_simple\"\n",
    "    \n",
    "def ask_continue(state):\n",
    "    user_input = input(\"계속 질문하시겠습니까? (y/n):\").strip()\n",
    "    state['continue'] = user_input.startswith('y')\n",
    "    return state\n",
    "\n",
    "\n",
    "def should_continue(stage):\n",
    "    return \"ask_question\" if stage.get('continue') else END\n",
    "\n",
    "\n",
    "\n",
    "graph = StateGraph(dict)\n",
    "graph.add_node(\"ask_question\", ask_question)\n",
    "graph.add_node(\"classify_question\", classify_question)\n",
    "graph.add_node(\"rag_simple\", rag_simple)\n",
    "graph.add_node(\"rag_iterative\", rag_iterative)\n",
    "graph.add_node(\"ask_continue\", ask_continue)\n",
    "\n",
    "graph.set_entry_point(\"ask_question\")\n",
    "\n",
    "graph.add_edge(\"ask_question\", \"classify_question\")\n",
    "graph.add_conditional_edges(\"classify_question\", route_strategy, {\n",
    "    \"rag_simple\": \"rag_simple\",\n",
    "    \"rag_iterative\": \"rag_iterative\",\n",
    "})\n",
    "\n",
    "graph.add_edge(\"rag_simple\", \"ask_continue\")\n",
    "graph.add_edge(\"rag_iterative\", \"ask_continue\")\n",
    "\n",
    "graph.add_conditional_edges(\"ask_continue\", should_continue, {\n",
    "    \"ask_question\": \"ask_question\",\n",
    "    END: END\n",
    "})\n",
    "\n",
    "session_id = str(uuid.uuid4())\n",
    "config = {\"configurable\": {\"session_id\": session_id}}\n",
    "app = graph.compile()\n",
    "app.invoke({}, config=config)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI2503",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
