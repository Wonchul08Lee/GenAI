{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe81189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from typing import List, Literal, Optional, TypedDict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langgraph.graph import StateGraph, END\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "#emdeding_id = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "emdeding_id = \"jhgan/ko-sbert-sts\"\n",
    "qa_model_id = \"monologg/koelectra-base-v3-finetuned-korquad\"\n",
    "#gen_model_id = \"EleutherAI/polyglot-ko-1.3b\"\n",
    "#gen_model_id = \"skt/kogpt2-base-v2\"\n",
    "gen_model_id = \"beomi/gemma-ko-2b\"\n",
    "\n",
    "tokenizer_opt = {\n",
    "    \"max_length\": 512, \n",
    "    \"truncation\": True,\n",
    "    \"do_sample\": True,\n",
    "    \"device\": -1\n",
    "}\n",
    "\n",
    "# 1. 문서 로드 및 VectorDB 저장\n",
    "filepath = \"./NewBie_개발환경가이드.docx\"\n",
    "persist_directory = \"./chromaDB_Chatbot\"\n",
    "#filepath = \"./data/data.csv\"\n",
    "#persist_directory = \"./ChromaDB_Chatbot_test\"\n",
    "\n",
    "\n",
    "def load_embedding_model():\n",
    "    return HuggingFaceEmbeddings(model_name=emdeding_id)\n",
    "\n",
    "def load_QA_model(tokenizer_opt):\n",
    "    qa_pipeline = pipeline(\n",
    "        task=\"question-answering\",\n",
    "        model=qa_model_id,\n",
    "        tokenizer_kwargs=tokenizer_opt,\n",
    "        device=-1 \n",
    "    )\n",
    "    return qa_pipeline\n",
    "\n",
    "def load_Gen_model():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(gen_model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(gen_model_id)\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "# Word 문서 로드 함수\n",
    "def load_word_documents(filepath: str):\n",
    "    loader = UnstructuredWordDocumentLoader(filepath)\n",
    "    docs = loader.load()\n",
    "    all_text = \"\\n\".join([doc.page_content for doc in docs])  # doc.page_content로 수정\n",
    "    sentences = re.split(r'\\.|\\n', all_text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    print(\"문장 개수:\", len(sentences))\n",
    "    print(\"샘플 문장 10개:\", sentences[:10])\n",
    "    print(\"마지막 10문장:\", sentences[-10:])\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def load_csv_documents(filepath):\n",
    "    df = pd.read_csv(filepath, encoding=\"utf-8\")\n",
    "    texts = df[\"text\"].tolist()\n",
    "    docs = [Document(page_content=text) for text in texts]\n",
    "    return docs\n",
    "\n",
    "# VectorDB 저장 함수\n",
    "def save_vectorDB(docs, persist_directory):\n",
    "    #embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "    embedding_model = load_embedding_model()\n",
    "\n",
    "    vector_db = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=persist_directory\n",
    ")\n",
    "    return vector_db\n",
    "\n",
    "# VectorDB에서 검색(retrieve) 함수\n",
    "def load_vectorDB(persist_directory):\n",
    "    #embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "    embedding_model = load_embedding_model()\n",
    "    vector_db = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embedding_model\n",
    ")\n",
    "    return vector_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccbcc5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qa(question, vector_db, qa_model_id, tokenizer_opt):\n",
    "    retriever = vector_db.as_retriever(search_kwargs={\"k\":5})\n",
    "    relevant_docs = retriever.invoke(question)\n",
    "    if len(relevant_docs) == 0:\n",
    "        print(\"검색 결과가 없습니다.\")\n",
    "        return None, None\n",
    "    # 여러 문서를 context로 합침 (길이 증가)\n",
    "    best_context = \"\\n\".join([doc.page_content for doc in relevant_docs[:5]])\n",
    "    print(\"[검색된 컨텍스트]\", best_context)\n",
    "    # qa_pipeline = pipeline(\n",
    "    #     task=\"question-answering\",\n",
    "    #     model=qa_model_id,\n",
    "    #     tokenizer_kwargs=tokenizer_opt,\n",
    "    #     device = -1\n",
    "    # )\n",
    "    qa_pipeline = load_QA_model(tokenizer_opt)\n",
    "    result = qa_pipeline(question=question,context=best_context)\n",
    "    print(\"[QA 답변]\", result['answer'])\n",
    "    return result, best_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "961d2fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 개수: 1905\n",
      "샘플 문장 10개: ['Pull Request 과정', '1', 'Git action flow 의 이해', 'file Pre-merge (Essential 통합)', 'github/workflows/01_premerge_pull_request', 'yaml premerge test 주관하는 workflow a', 'Integration test', '/', 'github/workflows/shared_premerge_integration_test', 'yaml b']\n",
      "마지막 10문장: ['커버리지 예시 (위의 workflow run중에 2번째)', '[AWIBOF-10103] add error id to log (#283) · poseidonos/pos-essential-orchestrator@2c322ac (github', 'com)', '클릭해서 가장 밑으로 스크롤하면 확인 가능', 'Trouble Shooting', '1', 'mockery 오류', '권한 문제', 'rsync등으로 코드만 미러링하여 해당 서버의 gopath에 필요한 패키지들이 없는 경우', 'go mod tidy 가 잘되면 문제없이 mockery 실행 가능']\n"
     ]
    }
   ],
   "source": [
    "#docs = load_csv_documents(filepath)\n",
    "sentences = load_word_documents(filepath)\n",
    "#docs = [Document(page_content=s) for s in sentences]  # 각 문장을 Document로 변환\n",
    "#save_vectorDB(docs, persist_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02382a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[검색된 컨텍스트] TDK는 다음과 같은 특징을 가지고 있습니다:\n",
      "TDK는 다음과 같은 특징을 가지고 있습니다:\n",
      "TDK는 이러한 문제를 해결하기 위해 만들어졌습니다\n",
      "TDK는 이러한 문제를 해결하기 위해 만들어졌습니다\n",
      "Test Development Kit (TDK)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA 답변] Test Development Kit\n"
     ]
    }
   ],
   "source": [
    "vector_db = load_vectorDB(persist_directory)\n",
    "question = \"TDK 는 무엇의 약자인가?\"\n",
    "result, best_context = run_qa(question, vector_db, qa_model_id, tokenizer_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e5afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb268c0300f549a5a4105651566be418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "당신은 Newbie가 아닌 시니어 개발자입니다.\n",
      "아래 내용을 참고하여 자연스럽고 명확한 한국어로 두세 문장으로 정리된 답변만 출력하세요.\n",
      "[참고 문서]\n",
      "TDK는 다음과 같은 특징을 가지고 있습니다:\n",
      "TDK는 이러한 문제를 해결하기 위해 만들어졌습니다\n",
      "Test Development Kit (TDK)\n",
      "[정답]\n",
      "Test Development Kit\n",
      "[최종 답변]\n",
      "TDK는 다음과 같이 사용됩니다:\n",
      "TDK는 다음과 같이 사용됩니다\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "import uuid\n",
    "\n",
    "\n",
    "def remove_duplicate_sentences(text):\n",
    "    sentences = text.split('\\n')\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for s in sentences:\n",
    "        s_strip = s.strip()\n",
    "        if s_strip and s_strip not in seen:\n",
    "            result.append(s_strip)\n",
    "            seen.add(s_strip)\n",
    "    return '\\n'.join(result)\n",
    "\n",
    "# 생성형 모델 준비\n",
    "\n",
    "#gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_id)\n",
    "#gen_model = AutoModelForCausalLM.from_pretrained(gen_model_id)\n",
    "gen_tokenizer, gen_model = load_Gen_model()\n",
    "gen_tokenizer.model_max_length = 1024\n",
    "gen_pipeline = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=gen_model,\n",
    "    tokenizer=gen_tokenizer,\n",
    "    max_new_tokens=200,\n",
    "    truncation=True,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.8,\n",
    "    repetition_penalty=1.2,\n",
    "    device=-1\n",
    ")\n",
    "gen_llm = HuggingFacePipeline(pipeline=gen_pipeline)\n",
    "\n",
    "\n",
    "# 프롬프트 템플릿 정의 (질문 반복 방지, 답변만 생성)\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "당신은 Newbie가 아닌 시니어 개발자입니다.                                     \n",
    "아래 내용을 참고하여 자연스럽고 명확한 한국어로 두세 문장으로 정리된 답변만 출력하세요.\n",
    "\n",
    "[참고 문서]\n",
    "{context}\n",
    "\n",
    "[정답]\n",
    "{answer}\n",
    "\n",
    "[최종 답변]\n",
    "\"\"\")\n",
    "\n",
    "chain = prompt | gen_llm\n",
    "\n",
    "final_response = chain.invoke({\n",
    "    \"question\": question,\n",
    "    \"context\": best_context,\n",
    "    \"answer\": result['answer']\n",
    "})\n",
    "final_response = remove_duplicate_sentences(final_response)\n",
    "final_response = \". \".join(list(dict.fromkeys(final_response.split(\". \"))))\n",
    "print(final_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI2503",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
