{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac70deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents have been saved to ChromaDB at ./chromaDB_Chatbot.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from typing import List, Literal, Optional, TypedDict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "def load_word_documents(filepath: str) :\n",
    "    loader = UnstructuredWordDocumentLoader(filepath)\n",
    "    docs = loader.load()\n",
    "    return docs\n",
    "\n",
    "def save_documents_to_chromadb(docs: list, persist_directory: str, embedding_model_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\") -> Chroma:\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=persist_directory\n",
    ")\n",
    "    print(f\"Documents have been saved to ChromaDB at {persist_directory}.\")\n",
    "    return vector_store\n",
    "\n",
    "# Example usage\n",
    "docs = load_word_documents(\"NewBie_개발환경가이드.docx\")\n",
    "vector_store = save_documents_to_chromadb(docs, persist_directory=\"./chromaDB_Chatbot\")\n",
    "\n",
    "\n",
    "# ---------- Graph State 정의 ----------\n",
    "def GraphState(TypedDict, total=False):\n",
    "    query: str\n",
    "    route: Literal[\"rag\"]              # 지금은 rag만 사용\n",
    "    retrieved_docs: List[str]\n",
    "    answer: str\n",
    "\n",
    "# ---------- 공통 LLM 생성 유틸 ----------\n",
    "def build_zephyr_llm():\n",
    "    rewrite_model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "    rewrite_token = AutoTokenizer.from_pretrained(rewrite_model_id)\n",
    "    rewrite_model = AutoModelForCausalLM.from_pretrained(rewrite_model_id)\n",
    "    rewrite_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=rewrite_model,\n",
    "        tokenizer=rewrite_token,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "    return HuggingFacePipeline(pipeline=rewrite_pipe)\n",
    "\n",
    "def build_llama3_llm():\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=mdl,\n",
    "        tokenizer=tok,\n",
    "        max_new_tokens=1024,\n",
    "    )\n",
    "    return HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "def build_retriever(chroma_persist_dir: str):\n",
    "    embed_model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    embedding = HuggingFaceEmbeddings(model_name=embed_model_name)\n",
    "\n",
    "    retriever = Chroma(\n",
    "        collection_name=\"onboarding_docs\",\n",
    "        persist_directory=chroma_persist_dir,\n",
    "        embedding_function=embedding,\n",
    "    )\n",
    "    return retriever.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "\n",
    "\n",
    "def supervisor_agent(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    입력: GraphState(query)\n",
    "    출력: GraphState(route)\n",
    "    현재는 단순히 모든 요청을 RAG로 라우팅.\n",
    "    나중에는 LLM을 써서 질의 유형 분석 후 분기 가능.\n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "\n",
    "    # 예시: LLM을 써서 route를 결정하는 형태 (지금은 주석 처리)\n",
    "    # prompt = f\"\"\"\n",
    "    # 너는 라우팅 에이전트야.\n",
    "    # 사용자의 질문이 단순 정보 질의라면 'rag'만 출력해.\n",
    "    # 질문: {query}\n",
    "    # 출력(딱 한 단어): \n",
    "    # \"\"\"\n",
    "    # route_text = supervisor_llm(prompt).strip()\n",
    "    # route = \"rag\" if \"rag\" in route_text else \"rag\"\n",
    "\n",
    "    route = \"rag\"  # 현재는 단일 경로\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"route\": route,\n",
    "    }\n",
    "\n",
    "def supervisor_node(state: GraphState):\n",
    "    return supervisor_agent(state)\n",
    "\n",
    "rewrite_llm = build_zephyr_llm()\n",
    "def rag_orchestrator_agent(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    - 질의를 검색에 적합하게 살짝 리라이팅(옵션)\n",
    "    - 지금은 단순히 query를 정제해서 그대로 넘김\n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "\n",
    "    # 간단한 query rewriting 예시 (LLM 사용 가능)\n",
    "    prompt = f\"\"\"\n",
    "    너는 검색용 쿼리 리라이팅 에이전트야.\n",
    "    아래 한국어/영어 혼합 질문을 RAG 검색에 적합한\n",
    "    짧은 한 문장의 쿼리로 바꿔줘.\n",
    "\n",
    "    질문: {query}\n",
    "    리라이팅된 검색 쿼리:\n",
    "    \"\"\"\n",
    "    rewritten = rewrite_llm(prompt)\n",
    "    rewritten_query = rewritten.strip()\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"query\": rewritten_query,  # 정제된 쿼리를 다음 노드에 전달\n",
    "    }\n",
    "\n",
    "def rag_orchestrator_node(state: GraphState):\n",
    "    return rag_orchestrator_agent(state)\n",
    "\n",
    "retriever = build_retriever(chroma_persist_dir=\"./chroma_onboarding\")\n",
    "def retrieval_agent(state: GraphState):\n",
    "    query = state[\"query\"]\n",
    "\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    contents: List[str] = [d.page_content for d in docs]\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"retrieved_docs\": contents,\n",
    "    }\n",
    "\n",
    "def retrieval_node(state: GraphState):\n",
    "    return retrieval_agent(state)\n",
    "\n",
    "\n",
    "answer_llm = build_llama3_llm()\n",
    "def answer_generation_agent(state: GraphState) -> GraphState:\n",
    "    query = state[\"query\"]\n",
    "    docs = state.get(\"retrieved_docs\", [])\n",
    "\n",
    "    context = \"\\n\\n---\\n\\n\".join(docs[:5])  # 너무 길면 상위 몇 개만 사용\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    너는 개발 환경 온보딩/트러블슈팅 도우미야.\n",
    "    아래는 Git, VSCode, Linux 등의 설정/문제 해결 관련 사내 문서 일부야.\n",
    "\n",
    "    [컨텍스트]\n",
    "    {context}\n",
    "\n",
    "    [사용자 질문]\n",
    "    {query}\n",
    "\n",
    "    위 컨텍스트를 우선적으로 사용해서,\n",
    "    한국어로 친절하고 구체적으로 답변해줘.\n",
    "    모르면 모른다고 말하고, 추측은 최소화해.\n",
    "    \"\"\"\n",
    "\n",
    "    answer = answer_llm(prompt)\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"answer\": answer.strip(),\n",
    "    }\n",
    "def answer_generation_node(state: GraphState) -> GraphState:\n",
    "    return answer_generation_agent(state)\n",
    "\n",
    "\n",
    "def build_graph():\n",
    "    workflow = StateGraph(GraphState)\n",
    "\n",
    "    workflow.add_node(\"supervisor\", supervisor_node)\n",
    "    #workflow.add_node(\"rag_orchestrator\", rag_orchestrator_node)\n",
    "    workflow.add_node(\"retrieval\", retrieval_node)\n",
    "    workflow.add_node(\"answer_generation\", answer_generation_node)\n",
    "\n",
    "    # 시작 노드\n",
    "    workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "    # supervisor → rag_orchestrator → retrieval → answer_generation → END)\n",
    "    workflow.add_edge(\"supervisor\", \"retrieval_node\")\n",
    "    #workflow.add_edge(\"rag_orchestrator\", \"retrieval\")\n",
    "    workflow.add_edge(\"retrieval\", \"answer_generation\")\n",
    "    workflow.add_edge(\"answer_generation\", END)\n",
    "\n",
    "    graph = workflow.compile()\n",
    "    return graph\n",
    "\n",
    "\n",
    "\n",
    "graph = build_graph()\n",
    "init_state: GraphState = {\"query\": \"VSCode에서 Python venv가 인식 안 될 때 설정 방법 알려줘\"}\n",
    "result = graph.invoke(init_state)\n",
    "print(result[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI2503",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
