[1일차]
inner product (내적) : 값이 높으면 유사도가 높다 

파라미터 : 데이터로부터 나오는 값
    w -> deep-learning은 찾아야할 파라미터가 많아서 높은 컴퓨팅 파워 요구
하이퍼 파라미터 : 사용자가 정하는 값  -> 트랜스포머는 이걸 튜닝하는것임


CNN -> 이미지, 영상
RNN -> Text 
            시퀀스 데이터(순서가 있는 데이터)
Auto Encoder : encoder/decoder 를 합쳐놓은것
  인코더 : 데이터가 들어왔을떄 정보를 압축
  디코더 : 정보를 푸는 작업
   
GAN 
   생성기 : 초기에는 허술했으나, 시간이 지나면서 발전
   판별기 : 초기에는 잘 판별하나, 생성기의 발전과 함께 발전
   생성기와 판결기가 적대적으로 발전
   다양한 이미지를 생성할 수 있음
   모델이 무거워(신경망) 많은 compute 필요

트랜스포머
   신경망을 뺴면서 딥러닝 대비 가벼워지고 빨라짐, 텍스트위주

디퓨전 : 그림 위주
트랜스포터, 디퓨전을 텍스트, 그림위주로 선택하는건 효율성 위주로 선택 

자연어 처리-> NLP
  시퀀스 데이터 
  자연어에 대한 처리가 매우 어려웠음 -> 각 나라의 말뭉치, 단어 꾸러미, 단어장들을 이용하여 해결
  -> 표준어만 입력 -> LLM (internet에서 단어 수집)

자연어 처리 기본 개념
- 확률 연산 -> 단어가 많고, 연산량이 많음
- 추론 기반 -> 앞뒤 문맥활용

word2vec
- chunk : 큰문서를 작업 문서로 분할
- tokenization : nice to meet you 
  "nice" => token
  임의 조건으로 나누는것( token) 화하는것을 tokenization
  [35, 27, 58, 61] => vector 화
- padding
  문장학습 시킬떄 문장의 길이가 동일해야 함. 동일한 token size !
- truncation 
   token 길이가 너무 길때, 제거
   제거된 token에 중요정보가 있을경우, 성능이 달라질 수 있으니, 튜닝이 필요
- embedding
   banana (21) -> computer는 이해할 수가 없어, embedding vector 로 표현
  -> [-0.3, 1.2 ,....] 로 하여 인식할 수 있도록 변경, embdedding vector의 길이는 모델에 따라 달라짐.
        embeddding model 은 선택/효율에 따라 다양하여 선택해야함. (수업은 384 길이의 모델 사용)
   -> vector들을 embedding 해서 행렬로 변경
  즉, 문장이 행렬이 되었다. 

RNN 
    앞에서 이해한 정보가 다음 step 의 입력으로 넘어감 
    오늘 날씨 어떄요? 
    (오늘) 로 how's -> (오늘+날씨) 로 the -> (오늘+날씨+어떄요) 로 weather 를 만든다.  
    가중치 행렬을 구해야함 (Wxh)
     (page 79)    
     (모델 b) : 문장 생성 , (nice) (to) (meet) ->  you
     (모델 c) : 문장 생성, 처음에 다 주어지는 경우 (옛날에, -> (토끼와 거북이가 살았어요))
     (모델 d) : how's the weather?

Perplexity
     성능 평가 할떄 사용하는 척도
     -> 문장이 자연스로운지 알 수 있음. 하지만, 이것으로만 되지는 않음
     -> 최종적으로는 사람이 판단하게 됨. 고급정보는 체점을 위해 전문 인력을 사용해야해서 인건비필요
 
LSTM
    RNN : 이전 결과가 누적 -> 뒤로 갈 수록 앞에 들었던 정보가 흐려짐. -> gradient vanishing
    다음 단계로 넘어갈떄, 장기 메모리/단기 메모리가 함께 전달
    삭제 게이트 : 중요 정보는 장기 기억으로, 중요하지 않은 정보는 단기 기억으로..(삭제 여부 결정) 
GRU 
    LSTM 의 복잡도를 낮춘 방식

양방향 LSTM 
 - 특정 시점 이후의 결과가 이전에 도움이 될 떄 사용-> 이해력이 높아짐
   정방향/양방향 결과를 기반으로 입력 x1 에 대한 y를 만듦. 
LSTM 단점
    input 과 출력 token 크기가 다르면 제대로 동작 안됨. -> seq2seq등장

seq2seq   -> audo encoder 구조 채용
 - encoder : "오늘 날씨 어떄요" 입력에 대한 이해. 
 - decoder : encoder에서 이해한 정보를 받아서, 생성하는 역할
   입/출력의 token 길이가 달라져도 무관
   <eos> 추가하여 문장이 끝났음을 알림. 이후 encoder 를 버무려서 하나의 정보로 만든것을 context vector 
 => encoder/decoder 에 가중치를 줄수 있다. GPT 는 decoder에 더 비중     

  [02_트랜스포머_논문_리뷰]
attention: 어디에 집중할 것이냐.
seq2seq 보완 위해 : 각단계에서 결과를 만들고 (context행렬), 이를 이용 
       각 단계에서 누적된 결과임 (h2는 (오늘, 날씨)의 결과가 조합)
       lstm 층이 여러단계일경우, 마지막 데이터만 사용
       lstm 층에서 context vector가 5개면, 다음 단계는 누구를 참조해야하나? (attention)
       attention scrore : 얼마나 집중해야하냐 가중치
       이 score는 어떻게 정하나? inner product값이 높으면 score가 높다. 
 
       softmax 사용 목적 -> 확률로 나타내기위해. (a1 + a2 + ... + a5= 1) 

       concatenate 한 후 원하는 결과가 아닐 경우, 선형 변환해서 줄인다. 
       w1 : 모델을 정의할때 임의 값으로 초기화, 학습과정에서 자동으로 최적화하는 가중치 행렬

[2일차]
도서관가서 많은 책들중, 누구에게 집중할 것인가? -> attention 
  Query 에 잘 맞는 key 로 value 를 찾는다 . -> transformer는 단어간의 관계를 찾는것이고, 이를 Query/key/value를 사용

  x (각 입력 단어) -> Q(질의)/K(특징)/V(data)를 만든다. => 모든 단어에 대해서 벡터 3개씩 준비

***************************
* 문제를 해결하기위해, encoder 기반, decoder 기반이 좋을지 판단해야한다. 
- 이해를 위주로 할지, 말을 많이 생성할지...
-> hugging face 에서 필요한 모델을 찾아야한다. 
    이 과정에서의 시행착오를 발표자료에 추가하는게 필요!!
